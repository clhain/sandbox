{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sandbox Documentation Background The Sandbox project is an opinionated set of apps that can be applied to a Kubernetes Cluster to give it some of the common functionality you might expect to find in a \"production-ready\" cluster (e.g. observability, ingress, gitops tooling, key vault, service mesh, etc), without all of the \"pesky\" rigor and strict policy enforcement. In other words, a development and experimentation sandbox that lives somewhere between a vanilla k8s cluster and one you should actually run user facing service on. Warning To Be Clear: while many of the necessary components are installed with some configuration, it must be stressed that this is NOT intended to carry production traffic. Quick Links Quick Start (Local install, partial feature set) Getting Started (Full feature set) Frequently Asked Questions What's Included A default Sandbox installation includes the following apps and services: App/Service Description ArgoCD Provides \"GitOps\" functionality and handles installation of Sandbox services Cert Manager Provides automated certificate management Grafana Visualize and explore telemetry from cluster and applications Loki Backend for storage and retrieval of container logs. NGINX Ingress Controller Provides cluster ingress services via NGINX Inc Ingress NGINX Mesh Provides service mesh functionality Oauth2 Proxy Uses common identity providers for authentication to ingress resources OPA Gatekeeper Define and enforce policies on your K8S infrastructure Opentelemetry Operator Collect and forward metrics / logs / traces from cluster services Prometheus Operator Collect metrics from cluster and installed applications Sealed Secrets Secured secrets for k8s infrastructure Tempo Backend for storage and retreival of distributed tracing data. A default Sandbox installation includes the following capabilities \"out of the box\": Container ingress services via NGINX Ingress Controller. Automated Let's Encrypt SSL certificate generation for ingress services (NGINX Ingress + Cert Manager). Oauth2 Authentication with common identity providers for users accessing ingress services (NGINX Ingress + Oauth2 Proxy). GitOps based application deployments (ArgoCD). Service mesh capabilities (NGINX Mesh). On-board telemetry collection and visualization (Grafana, Prometheus, Opentelemetry, Loki, Tempo). Policy Definition and Enforcement with Open Policy Agent Gatekeeper. Secure Secret Management patterns with Bitnami Sealed Secrets.","title":"Home"},{"location":"#sandbox-documentation","text":"","title":"Sandbox Documentation"},{"location":"#background","text":"The Sandbox project is an opinionated set of apps that can be applied to a Kubernetes Cluster to give it some of the common functionality you might expect to find in a \"production-ready\" cluster (e.g. observability, ingress, gitops tooling, key vault, service mesh, etc), without all of the \"pesky\" rigor and strict policy enforcement. In other words, a development and experimentation sandbox that lives somewhere between a vanilla k8s cluster and one you should actually run user facing service on. Warning To Be Clear: while many of the necessary components are installed with some configuration, it must be stressed that this is NOT intended to carry production traffic.","title":"Background"},{"location":"#quick-links","text":"Quick Start (Local install, partial feature set) Getting Started (Full feature set) Frequently Asked Questions","title":"Quick Links"},{"location":"#whats-included","text":"A default Sandbox installation includes the following apps and services: App/Service Description ArgoCD Provides \"GitOps\" functionality and handles installation of Sandbox services Cert Manager Provides automated certificate management Grafana Visualize and explore telemetry from cluster and applications Loki Backend for storage and retrieval of container logs. NGINX Ingress Controller Provides cluster ingress services via NGINX Inc Ingress NGINX Mesh Provides service mesh functionality Oauth2 Proxy Uses common identity providers for authentication to ingress resources OPA Gatekeeper Define and enforce policies on your K8S infrastructure Opentelemetry Operator Collect and forward metrics / logs / traces from cluster services Prometheus Operator Collect metrics from cluster and installed applications Sealed Secrets Secured secrets for k8s infrastructure Tempo Backend for storage and retreival of distributed tracing data. A default Sandbox installation includes the following capabilities \"out of the box\": Container ingress services via NGINX Ingress Controller. Automated Let's Encrypt SSL certificate generation for ingress services (NGINX Ingress + Cert Manager). Oauth2 Authentication with common identity providers for users accessing ingress services (NGINX Ingress + Oauth2 Proxy). GitOps based application deployments (ArgoCD). Service mesh capabilities (NGINX Mesh). On-board telemetry collection and visualization (Grafana, Prometheus, Opentelemetry, Loki, Tempo). Policy Definition and Enforcement with Open Policy Agent Gatekeeper. Secure Secret Management patterns with Bitnami Sealed Secrets.","title":"What's Included"},{"location":"faq/","text":"","title":"Frequently Asked Questions"},{"location":"troubleshooting/","text":"","title":"Troubleshooting"},{"location":"customization/default-services/","text":"","title":"Default services"},{"location":"customization/deploy-an-app/","text":"Deploy Your App To the Sandbox Background Once your Sandbox cluster is up and running (see Installation Guides ), one of the first things you might want to do is deploy your own application code. Containerization and authoring your the various Kubernetes manifests needed are too big of topics to cover here, so we'll focus our attention on the specific benefits available for you that Sandbox provides. Automated Deployment While you can deploy your Application kubernetes manifests manually (or using whatever deployment mechanism you prefer), the Sandbox cluster includes an ArgoCD instance you can use to automate the process. More information on the Sandbox installation of ArgoCD can be found in Services - ArgoCD . In general, the process for this is to define an ArgoCD Application that defines where / what to deploy, and then applying that to the cluster. Future updates to the target git repo will automatically propagate to the cluster. This example from the Examples Directory in the Sandbox GitHub project deploys an Application (from the app-helm directory of the examples folder) via helm: --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: boutique-app namespace: argocd spec: destination: server: https://kubernetes.default.svc namespace: default project: default source: repoURL: https://github.com/clhain/sandbox.git path: examples/simple-app-boutique/app-helm targetRevision: HEAD helm: parameters: # UPDATE WITH DESIRED HOST / DOMAIN INFO (the below will configure for app.example.com) - name: hostRecordName value: app - name: hostRecordDomain value: example.com syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true selfHeal: true In general, you can follow the pattern in that example directory for most simple deployments of the Sandbox and overlay apps: /cluster/ # The configs needed to deploy the cluster / Sandbox Apps. /app/ # Helm charts, kustomize, raw manifests to deploy /your-app.yaml # The ArgoCD Application manifest that tells the instance to deploy the /app/ directory contents. If you don't already have a strategy for managing the Secrets you're interested in applying to the cluster, make sure to check out the following section on Sealed Secrets, which is one way to do it that works very nicely with GitOps based App deployment. Secure Your Secrets Handling secure secret values in the Kubernetes world is a pretty broad topic with a lot of potential ways to deal with them. The Sandbox cluster includes Sealed Secrets from Bitnami Labs (but should work just fine with alternate methods should you prefer). More information on the Sandbox installation of Sealed Secrets can be found in Services - Sealed Secrets . To seal a secret, the process is roughly this: Install the kubeseal binary Follow The Usage Directions To Secure a Secret Note: Default helm install requires additional command line arguments to the kubeseal command as follows: kubeseal --controller-name sealed-secrets --controller-namespace sealed-secrets < /tmp/secret.json > sealed-secret.json Once your Sealed Secret is ready, you can check it in alongside your application code for deployment with ArgoCD as described above. Secure Your Ingress The Sandbox cluster has several key Ingress features that should work \"out of the box\". Automated Cert Provisioning The NGINX Ingress Controller is configured to interoperate with Cert Manager to automatically provision Let's Encrypt certs for ingresses. To enable this functionality, all you need to do is set the tls properties under \"spec\" as illustrated in this partial example for the cluster Grafana service. apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: grafana namespace: \"grafana\" spec: host: \"grafana.mysandbox.exampple.com\" ### This section here uses the letsencrypt-prod ClusterIssuer to provision a cert for ### grafana.mysandbox.example.com. The NGINX Ingress will automatically handle the HTTP ### challenge to verify the domain. You can also use the letsencrypt-staging Issuer if ### you're in development mode to avoid the 5 request / week throttling from LE. tls: cert-manager: cluster-issuer: letsencrypt-prod secret: grafana-cert ... Observe Your App Service Mesh","title":"Deploy Your App"},{"location":"customization/deploy-an-app/#deploy-your-app-to-the-sandbox","text":"","title":"Deploy Your App To the Sandbox"},{"location":"customization/deploy-an-app/#background","text":"Once your Sandbox cluster is up and running (see Installation Guides ), one of the first things you might want to do is deploy your own application code. Containerization and authoring your the various Kubernetes manifests needed are too big of topics to cover here, so we'll focus our attention on the specific benefits available for you that Sandbox provides.","title":"Background"},{"location":"customization/deploy-an-app/#automated-deployment","text":"While you can deploy your Application kubernetes manifests manually (or using whatever deployment mechanism you prefer), the Sandbox cluster includes an ArgoCD instance you can use to automate the process. More information on the Sandbox installation of ArgoCD can be found in Services - ArgoCD . In general, the process for this is to define an ArgoCD Application that defines where / what to deploy, and then applying that to the cluster. Future updates to the target git repo will automatically propagate to the cluster. This example from the Examples Directory in the Sandbox GitHub project deploys an Application (from the app-helm directory of the examples folder) via helm: --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: boutique-app namespace: argocd spec: destination: server: https://kubernetes.default.svc namespace: default project: default source: repoURL: https://github.com/clhain/sandbox.git path: examples/simple-app-boutique/app-helm targetRevision: HEAD helm: parameters: # UPDATE WITH DESIRED HOST / DOMAIN INFO (the below will configure for app.example.com) - name: hostRecordName value: app - name: hostRecordDomain value: example.com syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true selfHeal: true In general, you can follow the pattern in that example directory for most simple deployments of the Sandbox and overlay apps: /cluster/ # The configs needed to deploy the cluster / Sandbox Apps. /app/ # Helm charts, kustomize, raw manifests to deploy /your-app.yaml # The ArgoCD Application manifest that tells the instance to deploy the /app/ directory contents. If you don't already have a strategy for managing the Secrets you're interested in applying to the cluster, make sure to check out the following section on Sealed Secrets, which is one way to do it that works very nicely with GitOps based App deployment.","title":"Automated Deployment"},{"location":"customization/deploy-an-app/#secure-your-secrets","text":"Handling secure secret values in the Kubernetes world is a pretty broad topic with a lot of potential ways to deal with them. The Sandbox cluster includes Sealed Secrets from Bitnami Labs (but should work just fine with alternate methods should you prefer). More information on the Sandbox installation of Sealed Secrets can be found in Services - Sealed Secrets . To seal a secret, the process is roughly this: Install the kubeseal binary Follow The Usage Directions To Secure a Secret Note: Default helm install requires additional command line arguments to the kubeseal command as follows: kubeseal --controller-name sealed-secrets --controller-namespace sealed-secrets < /tmp/secret.json > sealed-secret.json Once your Sealed Secret is ready, you can check it in alongside your application code for deployment with ArgoCD as described above.","title":"Secure Your Secrets"},{"location":"customization/deploy-an-app/#secure-your-ingress","text":"The Sandbox cluster has several key Ingress features that should work \"out of the box\".","title":"Secure Your Ingress"},{"location":"customization/deploy-an-app/#automated-cert-provisioning","text":"The NGINX Ingress Controller is configured to interoperate with Cert Manager to automatically provision Let's Encrypt certs for ingresses. To enable this functionality, all you need to do is set the tls properties under \"spec\" as illustrated in this partial example for the cluster Grafana service. apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: grafana namespace: \"grafana\" spec: host: \"grafana.mysandbox.exampple.com\" ### This section here uses the letsencrypt-prod ClusterIssuer to provision a cert for ### grafana.mysandbox.example.com. The NGINX Ingress will automatically handle the HTTP ### challenge to verify the domain. You can also use the letsencrypt-staging Issuer if ### you're in development mode to avoid the 5 request / week throttling from LE. tls: cert-manager: cluster-issuer: letsencrypt-prod secret: grafana-cert ...","title":"Automated Cert Provisioning"},{"location":"customization/deploy-an-app/#observe-your-app","text":"","title":"Observe Your App"},{"location":"customization/deploy-an-app/#service-mesh","text":"","title":"Service Mesh"},{"location":"customization/service-dependencies/","text":"Service Dependencies The default Sandbox Services have the following inter-dependencies. When removing or replacing components, the dependencies must be accounted for. Service Depends On For All Services ArgoCD Deployment and configuration via sandbox-apps helm chart. Grafana NGINX Ingress Cluster Ingress service Grafana Oauth2 Proxy User authentication","title":"Service Dependencies"},{"location":"customization/service-dependencies/#service-dependencies","text":"The default Sandbox Services have the following inter-dependencies. When removing or replacing components, the dependencies must be accounted for. Service Depends On For All Services ArgoCD Deployment and configuration via sandbox-apps helm chart. Grafana NGINX Ingress Cluster Ingress service Grafana Oauth2 Proxy User authentication","title":"Service Dependencies"},{"location":"installation/all-in-one-gke/","text":"All-In-One Installation On GKE The All-In-One installation uses Porter based CNAB bundles to deploy the cluster and Sandbox applications / services in a single command. If you're not familiar with Porter/CNAB, think of it as a docker container for your deployment pipelines - allowing you to compose multiple tools (e.g. helm, terraform, bash) into a single image with a common interface, that can be versioned, etc, just like an application container. In this case, the bundle does 2 things: Invoke terraform to provision a Private GKE Cluster with the parameters passed to it via Porter. Invoke Helm to install the sandbox-base chart (which installs ArgoCD + Sandbox Argo App definitions) on top of the cluster from #1. Below assumes a non-customized installation of the bundle. For more information on customizing included services, see Customizing Default Services . 1. Install Porter Follow the Porter Installation Guide to install the recommended v1+ release on the machine which you wish to deploy the bundle from. 2. GKE Specific Pre-Reqs You'll need the following available prior to deployment of the All-In-One GKE Sandbox Bundle in order to deploy the cluster: A GCP Project ( Instructions ). A Service Account and json key to Use For Deploying the cluster (see Porter Deployer Service Account below). (Optional) - A pre-existing Service Account name (key not required) to assign to the deployed nodes in the cluster (see Cluster Node Service Account below). Porter Deployer Service Account A GCP service account JSON key is required to deploy the bundle. Required permissions for the deployer Service Account are found in a custom role definition file deployer_role.yaml . You can also use the following Built-In Roles: Compute Admin Compute Network Admin Kubernetes Engine Admin Kubernetes Engine Cluster Admin Service Account User If you're not using the default compute engine service account or bringing a pre-existing SA, the deployer will also need Service Account Creator permissions. Example account creation and permission commands (must be run by a user with permissions to create service accounts): gcloud iam service-accounts create porter-deployer-sa \\ --description=\"Service Account Used with Porter to deploy GCP assets\" \\ --display-name=\"Porter Deploy Service Account\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/iam.serviceAccountUser\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/container.admin\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/container.clusterAdmin\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/compute.networkAdmin\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/compute.admin\" If you're not using the default compute engine service account or bringing a pre-existing SA: gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/iam.serviceAccountCreator\" Cluster Node Service Account (Optional) If you elect to use the default compute engine service account (not required), or want to use a pre-existing service account for the Node Pool, you can specify it as a parameter to the installation bundle. The name of the service account can be passed as a parameter, no json key is required. If the cluster needs access to Google Container Registry associated with the project, add the Storage Object Reader permission (or permissions for any other GCP resources to be accessed by the nodes). Example gcloud commands: gcloud iam service-accounts create sandbox-cluster-1-node-sa \\ --description=\"Service Account For GKE Cluster Nodes\" \\ --display-name=\"Cluster Node Service Account\" # Optional additional permissions gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:sandbox-cluster-1-node-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/storage.objectViewer\" 3. Sandbox Common Pre-Reqs In addition to the GCP specific pre-reqs, you'll also need the following for all default functionality: A subdomain to use with the deployment and access to create DNS records. An email address to use as contact info for Let's Encrypt Certificates. OIDC Client information from your identity provider. Read more about these in the dedicated Common Pre-Reqs page. 4. Configure Porter We strongly recommend managing porter installations via the \"Desired State\" mode . With this mode, you'll configure a few files that specify where porter can find the parameters and credentials needed to perform the deployment, and then invoke the installation itself through a series of 'Apply' commands. We recommend setting up a new project with cluster specification as follows: your_project/ cluster/ creds.yaml # Credentials spec for bundle installation params.yaml # Parameters spec for bundle installation installation.yaml # Installation spec sandbox-values.yaml # Helm values to pass to sandbox-base helm chart installation You can copy the files in the Example Cluster Directories and modify them as needed. Porter Credentials File The creds.yaml file contains references to where porter can find the sensitive parameters (e.g. passwords, keys, etc) needed to perform the install. An example declaration might look like this: schemaVersion: 1.0.1 name: sandbox-cluster-1 # The name of the porter credential set that will be created credentials: - name: gcloud_key_file # The name of the credential that is passed to the bundle source: # Where to find the value for the credential path: /tmp/cloud.key # This specifies that the gcloud-key-file parameter comes from the path /tmp/cloud.key - name: oidc_client_secret # The name of the credential that is passed to the bundle source: # Where to find the value for the credential env: OIDC_CLIENT_SECRET # This specifies that the oidc_client_secret parameter comes an environment variable named OIDC_CLIENT_SECRET Place your Deployment Service Account Key from step 2 above in /tmp/gcloud.key (or the location of your choice, updating the file). Set the value of OIDC_CLIENT_SECRET to your key value from step 3 above (not required if using the default Dex identity provider) export OIDC_CLIENT_SECRET=YOUR_CLIENT_SECRET_VALUE Porter Parameters File The params.yaml file contains references to non-sensitive configuration values to use for bundle installation. An example declaration might look like this: schemaVersion: 1.0.1 name: sandbox-cluster-1 parameters: - name: cluster_domain source: value: #### YOUR_DOMAIN_HERE #### - name: gcp_project source: value: #### YOUR_GCP_PROJECT_ID_HERE #### - name: oidc_permitted_email_domains source: value: #### YOUR_EMAIL_DOMAIN_ALLOW_LIST_HERE (e.g. f5.com) #### - name: oidc_client_id source: value: #### YOUR_OIDC_CLIENT_ID_HERE #### - name: oidc_issuer_url source: value: #### YOUR_OIDC_ISSUER_URL_HERE #### - name: letsencrypt_contact_email source: value: #### YOUR_CONTACT_EMAIL_HERE #### - name: sandbox-values source: path: ./sandbox-values.yaml Replace all instances of #### YOUR_X_HERE #### with the values obtained in steps 2 and 3 above. You can remove all oidc_ values if you're using the included local dex IDP. You can remove letsencrypt_contact_email if you've set clusterTLSEnabled=false. You can remove sandbox-values if you're not customizing helm values directly. For additional parameters supported by the bundle, you can run porter explain ghcr.io/clhain/sandbox-gke-porter:v0.1.7 from the command line. Porter Installation File The installation.yaml file contains information about what credentials and parameters should be applied to a given installation of a particular bundle. If you re-named the credential or parameter sets from the values above, you'll need to update these values, otherwise the below should work without modification: schemaVersion: 1.0.1 name: sandbox-cluster-1 bundle: repository: ghcr.io/clhain/sandbox-gke-porter version: v0.1.7 parameterSets: - sandbox-cluster-1 credentialSets: - sandbox-cluster-1 Sandbox Values File The sandbox-values.yaml file contains additional helm values to pass to the installation of the sandbox-base charts, which are deployed automatically once the cluster is installed. The helm chart installs argo-cd to the new cluster, and then adds an ArgoCD Application spec which handles deployment of all Sandbox applications / services. See the Default Service Configuration guide for more info. 5. Install The Bundle Once the params file is updated and the credentials are put in place (see above), the installation is straightforward: porter creds apply cluster/creds.yaml porter parameters apply cluster/params.yaml porter installations apply cluster/installation.yaml The GKE cluster will be deployed, and the initial Argo Installation will occur. Following that, the individual sandbox components will be installed by ArgoCD over the next 10-20 minutes. You can track the installation progress after connecting to the cluster with: kubectl get application -n argocd Once you see the following, you should be good to go (see Verification below for more info): NAME SYNC STATUS HEALTH STATUS argo-virtual-server Synced Healthy cert-manager Synced Healthy grafana Synced Healthy loki Synced Healthy nginx-ingress Synced Healthy nginx-mesh Synced Healthy oauth-proxy Synced Healthy opentelemetry-operator Synced Healthy prometheus-operator Synced Healthy prometheus-operator-crds Synced Healthy sandbox-apps Synced Healthy temppo Synced Healthy 6. Verification If you have the argocd CLI utility installed , you can use it to view additional information and troubleshoot issues as needed: 1) Fetch the ArgoCD Admin Password kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo 2) Port Forward ArgoCD CLI Commands export ARGOCD_OPTS='--port-forward-namespace argocd' 3) Login To The ArgoCD Instance argocd login --port-forward --insecure 4) View App Rollout Progress argocd app get sandbox-apps When complete you should see a list that looks like this (all items are Synced and Healthy/blank): GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Secret oauth-proxy dex-config-secret Synced secret/dex-config-secret configured argoproj.io Application argocd cert-manager Synced Healthy application.argoproj.io/cert-manager configured argoproj.io Application argocd nginx-ingress Synced Healthy application.argoproj.io/nginx-ingress configured ConfigMap kube-system coredns Synced Namespace oauth-proxy Synced Secret oauth-proxy oauth-proxy-creds Synced argoproj.io AppProject argocd cluster-services Synced argoproj.io Application argocd argo-virtual-server Synced Healthy argoproj.io Application argocd gatekeeper Synced Healthy argoproj.io Application argocd grafana Synced Healthy argoproj.io Application argocd loki Synced Healthy argoproj.io Application argocd nginx-mesh Synced Healthy argoproj.io Application argocd oauth-proxy Synced Healthy argoproj.io Application argocd opentelemetry-operator Synced Healthy argoproj.io Application argocd prometheus-operator Synced Healthy argoproj.io Application argocd prometheus-operator-crds Synced Healthy argoproj.io Application argocd sealed-secrets Synced Healthy argoproj.io Application argocd tempo Synced Healthy 7. Access Sandbox Services Once all services are in 'Synced, Healthy' state, and you've updated the DNS records as described here , you should be able to securely access the ArgoCD and Grafana services at: https://argocd.YOUR_DOMAIN/ https://grafana.YOUR_DOMAIN/ If you're using the default Dex IDP, you can fetch the credentials for the admin@example.com domain from the cluster as follows: kubectl get secret -n oauth-proxy oauth-proxy-creds -o jsonpath=\"{.data.admin-password}\" | base64 -d; echo Please see the troubleshooting guide for investigating issues.","title":"All-In-One (GKE)"},{"location":"installation/all-in-one-gke/#all-in-one-installation-on-gke","text":"The All-In-One installation uses Porter based CNAB bundles to deploy the cluster and Sandbox applications / services in a single command. If you're not familiar with Porter/CNAB, think of it as a docker container for your deployment pipelines - allowing you to compose multiple tools (e.g. helm, terraform, bash) into a single image with a common interface, that can be versioned, etc, just like an application container. In this case, the bundle does 2 things: Invoke terraform to provision a Private GKE Cluster with the parameters passed to it via Porter. Invoke Helm to install the sandbox-base chart (which installs ArgoCD + Sandbox Argo App definitions) on top of the cluster from #1. Below assumes a non-customized installation of the bundle. For more information on customizing included services, see Customizing Default Services .","title":"All-In-One Installation On GKE"},{"location":"installation/all-in-one-gke/#1-install-porter","text":"Follow the Porter Installation Guide to install the recommended v1+ release on the machine which you wish to deploy the bundle from.","title":"1. Install Porter"},{"location":"installation/all-in-one-gke/#2-gke-specific-pre-reqs","text":"You'll need the following available prior to deployment of the All-In-One GKE Sandbox Bundle in order to deploy the cluster: A GCP Project ( Instructions ). A Service Account and json key to Use For Deploying the cluster (see Porter Deployer Service Account below). (Optional) - A pre-existing Service Account name (key not required) to assign to the deployed nodes in the cluster (see Cluster Node Service Account below).","title":"2. GKE Specific Pre-Reqs"},{"location":"installation/all-in-one-gke/#porter-deployer-service-account","text":"A GCP service account JSON key is required to deploy the bundle. Required permissions for the deployer Service Account are found in a custom role definition file deployer_role.yaml . You can also use the following Built-In Roles: Compute Admin Compute Network Admin Kubernetes Engine Admin Kubernetes Engine Cluster Admin Service Account User If you're not using the default compute engine service account or bringing a pre-existing SA, the deployer will also need Service Account Creator permissions. Example account creation and permission commands (must be run by a user with permissions to create service accounts): gcloud iam service-accounts create porter-deployer-sa \\ --description=\"Service Account Used with Porter to deploy GCP assets\" \\ --display-name=\"Porter Deploy Service Account\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/iam.serviceAccountUser\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/container.admin\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/container.clusterAdmin\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/compute.networkAdmin\" gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/compute.admin\" If you're not using the default compute engine service account or bringing a pre-existing SA: gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:porter-deployer-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/iam.serviceAccountCreator\"","title":"Porter Deployer Service Account"},{"location":"installation/all-in-one-gke/#cluster-node-service-account-optional","text":"If you elect to use the default compute engine service account (not required), or want to use a pre-existing service account for the Node Pool, you can specify it as a parameter to the installation bundle. The name of the service account can be passed as a parameter, no json key is required. If the cluster needs access to Google Container Registry associated with the project, add the Storage Object Reader permission (or permissions for any other GCP resources to be accessed by the nodes). Example gcloud commands: gcloud iam service-accounts create sandbox-cluster-1-node-sa \\ --description=\"Service Account For GKE Cluster Nodes\" \\ --display-name=\"Cluster Node Service Account\" # Optional additional permissions gcloud projects add-iam-policy-binding PROJECT_ID \\ --member=\"serviceAccount:sandbox-cluster-1-node-sa@PROJECT_ID.iam.gserviceaccount.com\" \\ --role=\"roles/storage.objectViewer\"","title":"Cluster Node Service Account (Optional)"},{"location":"installation/all-in-one-gke/#3-sandbox-common-pre-reqs","text":"In addition to the GCP specific pre-reqs, you'll also need the following for all default functionality: A subdomain to use with the deployment and access to create DNS records. An email address to use as contact info for Let's Encrypt Certificates. OIDC Client information from your identity provider. Read more about these in the dedicated Common Pre-Reqs page.","title":"3. Sandbox Common Pre-Reqs"},{"location":"installation/all-in-one-gke/#4-configure-porter","text":"We strongly recommend managing porter installations via the \"Desired State\" mode . With this mode, you'll configure a few files that specify where porter can find the parameters and credentials needed to perform the deployment, and then invoke the installation itself through a series of 'Apply' commands. We recommend setting up a new project with cluster specification as follows: your_project/ cluster/ creds.yaml # Credentials spec for bundle installation params.yaml # Parameters spec for bundle installation installation.yaml # Installation spec sandbox-values.yaml # Helm values to pass to sandbox-base helm chart installation You can copy the files in the Example Cluster Directories and modify them as needed.","title":"4. Configure Porter"},{"location":"installation/all-in-one-gke/#porter-credentials-file","text":"The creds.yaml file contains references to where porter can find the sensitive parameters (e.g. passwords, keys, etc) needed to perform the install. An example declaration might look like this: schemaVersion: 1.0.1 name: sandbox-cluster-1 # The name of the porter credential set that will be created credentials: - name: gcloud_key_file # The name of the credential that is passed to the bundle source: # Where to find the value for the credential path: /tmp/cloud.key # This specifies that the gcloud-key-file parameter comes from the path /tmp/cloud.key - name: oidc_client_secret # The name of the credential that is passed to the bundle source: # Where to find the value for the credential env: OIDC_CLIENT_SECRET # This specifies that the oidc_client_secret parameter comes an environment variable named OIDC_CLIENT_SECRET Place your Deployment Service Account Key from step 2 above in /tmp/gcloud.key (or the location of your choice, updating the file). Set the value of OIDC_CLIENT_SECRET to your key value from step 3 above (not required if using the default Dex identity provider) export OIDC_CLIENT_SECRET=YOUR_CLIENT_SECRET_VALUE","title":"Porter Credentials File"},{"location":"installation/all-in-one-gke/#porter-parameters-file","text":"The params.yaml file contains references to non-sensitive configuration values to use for bundle installation. An example declaration might look like this: schemaVersion: 1.0.1 name: sandbox-cluster-1 parameters: - name: cluster_domain source: value: #### YOUR_DOMAIN_HERE #### - name: gcp_project source: value: #### YOUR_GCP_PROJECT_ID_HERE #### - name: oidc_permitted_email_domains source: value: #### YOUR_EMAIL_DOMAIN_ALLOW_LIST_HERE (e.g. f5.com) #### - name: oidc_client_id source: value: #### YOUR_OIDC_CLIENT_ID_HERE #### - name: oidc_issuer_url source: value: #### YOUR_OIDC_ISSUER_URL_HERE #### - name: letsencrypt_contact_email source: value: #### YOUR_CONTACT_EMAIL_HERE #### - name: sandbox-values source: path: ./sandbox-values.yaml Replace all instances of #### YOUR_X_HERE #### with the values obtained in steps 2 and 3 above. You can remove all oidc_ values if you're using the included local dex IDP. You can remove letsencrypt_contact_email if you've set clusterTLSEnabled=false. You can remove sandbox-values if you're not customizing helm values directly. For additional parameters supported by the bundle, you can run porter explain ghcr.io/clhain/sandbox-gke-porter:v0.1.7 from the command line.","title":"Porter Parameters File"},{"location":"installation/all-in-one-gke/#porter-installation-file","text":"The installation.yaml file contains information about what credentials and parameters should be applied to a given installation of a particular bundle. If you re-named the credential or parameter sets from the values above, you'll need to update these values, otherwise the below should work without modification: schemaVersion: 1.0.1 name: sandbox-cluster-1 bundle: repository: ghcr.io/clhain/sandbox-gke-porter version: v0.1.7 parameterSets: - sandbox-cluster-1 credentialSets: - sandbox-cluster-1","title":"Porter Installation File"},{"location":"installation/all-in-one-gke/#sandbox-values-file","text":"The sandbox-values.yaml file contains additional helm values to pass to the installation of the sandbox-base charts, which are deployed automatically once the cluster is installed. The helm chart installs argo-cd to the new cluster, and then adds an ArgoCD Application spec which handles deployment of all Sandbox applications / services. See the Default Service Configuration guide for more info.","title":"Sandbox Values File"},{"location":"installation/all-in-one-gke/#5-install-the-bundle","text":"Once the params file is updated and the credentials are put in place (see above), the installation is straightforward: porter creds apply cluster/creds.yaml porter parameters apply cluster/params.yaml porter installations apply cluster/installation.yaml The GKE cluster will be deployed, and the initial Argo Installation will occur. Following that, the individual sandbox components will be installed by ArgoCD over the next 10-20 minutes. You can track the installation progress after connecting to the cluster with: kubectl get application -n argocd Once you see the following, you should be good to go (see Verification below for more info): NAME SYNC STATUS HEALTH STATUS argo-virtual-server Synced Healthy cert-manager Synced Healthy grafana Synced Healthy loki Synced Healthy nginx-ingress Synced Healthy nginx-mesh Synced Healthy oauth-proxy Synced Healthy opentelemetry-operator Synced Healthy prometheus-operator Synced Healthy prometheus-operator-crds Synced Healthy sandbox-apps Synced Healthy temppo Synced Healthy","title":"5. Install The Bundle"},{"location":"installation/all-in-one-gke/#6-verification","text":"If you have the argocd CLI utility installed , you can use it to view additional information and troubleshoot issues as needed: 1) Fetch the ArgoCD Admin Password kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo 2) Port Forward ArgoCD CLI Commands export ARGOCD_OPTS='--port-forward-namespace argocd' 3) Login To The ArgoCD Instance argocd login --port-forward --insecure 4) View App Rollout Progress argocd app get sandbox-apps When complete you should see a list that looks like this (all items are Synced and Healthy/blank): GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Secret oauth-proxy dex-config-secret Synced secret/dex-config-secret configured argoproj.io Application argocd cert-manager Synced Healthy application.argoproj.io/cert-manager configured argoproj.io Application argocd nginx-ingress Synced Healthy application.argoproj.io/nginx-ingress configured ConfigMap kube-system coredns Synced Namespace oauth-proxy Synced Secret oauth-proxy oauth-proxy-creds Synced argoproj.io AppProject argocd cluster-services Synced argoproj.io Application argocd argo-virtual-server Synced Healthy argoproj.io Application argocd gatekeeper Synced Healthy argoproj.io Application argocd grafana Synced Healthy argoproj.io Application argocd loki Synced Healthy argoproj.io Application argocd nginx-mesh Synced Healthy argoproj.io Application argocd oauth-proxy Synced Healthy argoproj.io Application argocd opentelemetry-operator Synced Healthy argoproj.io Application argocd prometheus-operator Synced Healthy argoproj.io Application argocd prometheus-operator-crds Synced Healthy argoproj.io Application argocd sealed-secrets Synced Healthy argoproj.io Application argocd tempo Synced Healthy","title":"6. Verification"},{"location":"installation/all-in-one-gke/#7-access-sandbox-services","text":"Once all services are in 'Synced, Healthy' state, and you've updated the DNS records as described here , you should be able to securely access the ArgoCD and Grafana services at: https://argocd.YOUR_DOMAIN/ https://grafana.YOUR_DOMAIN/ If you're using the default Dex IDP, you can fetch the credentials for the admin@example.com domain from the cluster as follows: kubectl get secret -n oauth-proxy oauth-proxy-creds -o jsonpath=\"{.data.admin-password}\" | base64 -d; echo Please see the troubleshooting guide for investigating issues.","title":"7. Access Sandbox Services"},{"location":"installation/byoc-argocd/","text":"Bring Your Own Cluster (ArgoCD) If you already have a cluster ready to deploy the Sandbox apps set to, you can do so with ArgoCD directly if it's already installed (if not, see the Helm Based Install ) Note: It's recommended not to have existing services on the destination sandbox cluster to avoid collisions or other compatibility issues. ArgoCD should be safe however, if you wish to manage it via some other process. 0. Pre-Requisites See the guide for Common Sandbox Pre-Reqs for the set of parameters you'll need to have available. In short, they include: clusterDomain: # e.g. yourdomain.com clusterIngressIP: # e.g. 1.2.3.4 oidcClientID: # e.g. some-client-id oidcClientSecret: # e.g. some-secret-value oidcIssuerURL: # e.g. \"https://login.microsoftonline.com/TENANT/v2.0\" oidcPermittedEmailDomains: # e.g. \"yourdomain.com\" letsEncryptContactEmail: # e.g. \"someone@yourdomain.com\" 1. Configure The Application.yaml We need to pass an ArgoCD Application spec to the cluster that details how to find and deploy the sandbox services. Copy the Example Bring Your Own Cluster Spec (Argo) and modify as needed. The values look like this, and the spec.source.helm.parameters must be set based on the values gathered in step 0: --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: sandbox-cluster-apps namespace: argocd ## YOUR ARGOCD NAMESPACE spec: destination: server: https://kubernetes.default.svc namespace: argocd ## YOUR ARGOCD NAMESPACE project: default ## YOUR ARGOCD PROJECT source: repoURL: https://github.com/clhain/sandbox-helm-charts.git path: charts/sandbox-apps targetRevision: HEAD helm: parameters: - name: clusterDomain value: # YOUR CLUSTER DOMAIN (e.g. example.com) - name: clusterIngressIP value: # YOUR CLUSTER Ingress IP (e.g. 1.2.3.4) - name: oidcClientID value: # YOUR OIDC Client ID String - name: oidcClientSecret value: # YOUR OIDC Client Secret String - name: oidcIssuerURL value: # YOUR OIDC ISSUER URL (e.g. \"https://login.microsoftonline.com/TENANT/v2.0\") - name: oidcPermittedEmailDomains value: \"*\" # YOUR PERMITTED OIDC EMAIL DOMAINS (e.g. example.com) - name: letsEncryptContactEmail value: # YOUR LETS ENCRYPT CONTACT EMAIL (e.g. nobody@example.com) - name apps.argo-virtual-server.enabled value: false # Turn off the ArgoCD Virtual server assuming it's already exposed via manual process. syncPolicy: automated: prune: true selfHeal: true Additional customization is possible via the helm parameters. See Customizing Default Services for more info. 2. Install The Sandbox ArgoCD Application kubectl apply -f /path/to/your/application.yaml Following that, the individual sandbox components will be installed by ArgoCD over the next 10-20 minutes. You can track the installation progress after connecting to the cluster with: kubectl get application -n argocd Once you see the following, you should be good to go: NAME SYNC STATUS HEALTH STATUS argo-virtual-server Synced Healthy cert-manager Synced Healthy grafana Synced Healthy loki Synced Healthy nginx-ingress Synced Healthy nginx-mesh Synced Healthy oauth-proxy Synced Healthy opentelemetry-operator Synced Healthy prometheus-operator Synced Healthy prometheus-operator-crds Synced Healthy sandbox-apps Synced Healthy temppo Synced Healthy 3. Verification If you have the argocd CLI utility installed , you can use it to view additional information and troubleshoot issues as needed: 1) Fetch the ArgoCD Admin Password kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo 2) Port Forward ArgoCD CLI Commands export ARGOCD_OPTS='--port-forward-namespace argocd' 3) Login To The ArgoCD Instance argocd login --port-forward --insecure 4) View App Rollout Progress argocd app get sandbox-apps When complete you should see a list that looks like this (all items are Synced and Healthy/blank): GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Secret oauth-proxy dex-config-secret Synced secret/dex-config-secret configured argoproj.io Application argocd cert-manager Synced Healthy application.argoproj.io/cert-manager configured argoproj.io Application argocd nginx-ingress Synced Healthy application.argoproj.io/nginx-ingress configured ConfigMap kube-system coredns Synced Namespace oauth-proxy Synced Secret oauth-proxy oauth-proxy-creds Synced argoproj.io AppProject argocd cluster-services Synced argoproj.io Application argocd argo-virtual-server Synced Healthy argoproj.io Application argocd gatekeeper Synced Healthy argoproj.io Application argocd grafana Synced Healthy argoproj.io Application argocd loki Synced Healthy argoproj.io Application argocd nginx-mesh Synced Healthy argoproj.io Application argocd oauth-proxy Synced Healthy argoproj.io Application argocd opentelemetry-operator Synced Healthy argoproj.io Application argocd prometheus-operator Synced Healthy argoproj.io Application argocd prometheus-operator-crds Synced Healthy argoproj.io Application argocd sealed-secrets Synced Healthy argoproj.io Application argocd tempo Synced Healthy 4. Access Sandbox Services Once all services are in 'Synced, Healthy' state, and you've updated the DNS records as described here , you should be able to securely access the ArgoCD and Grafana services at: https://argocd.YOUR_DOMAIN/ https://grafana.YOUR_DOMAIN/ If you're using the default Dex IDP, you can fetch the credentials for the admin@example.com domain from the cluster as follows: kubectl get secret -n oauth-proxy oauth-proxy-creds -o jsonpath=\"{.data.admin-password}\" | base64 -d; echo Please see the troubleshooting guide for investigating issues.","title":"Bring Your Own Cluster (ArgoCD)"},{"location":"installation/byoc-argocd/#bring-your-own-cluster-argocd","text":"If you already have a cluster ready to deploy the Sandbox apps set to, you can do so with ArgoCD directly if it's already installed (if not, see the Helm Based Install ) Note: It's recommended not to have existing services on the destination sandbox cluster to avoid collisions or other compatibility issues. ArgoCD should be safe however, if you wish to manage it via some other process.","title":"Bring Your Own Cluster (ArgoCD)"},{"location":"installation/byoc-argocd/#0-pre-requisites","text":"See the guide for Common Sandbox Pre-Reqs for the set of parameters you'll need to have available. In short, they include: clusterDomain: # e.g. yourdomain.com clusterIngressIP: # e.g. 1.2.3.4 oidcClientID: # e.g. some-client-id oidcClientSecret: # e.g. some-secret-value oidcIssuerURL: # e.g. \"https://login.microsoftonline.com/TENANT/v2.0\" oidcPermittedEmailDomains: # e.g. \"yourdomain.com\" letsEncryptContactEmail: # e.g. \"someone@yourdomain.com\"","title":"0. Pre-Requisites"},{"location":"installation/byoc-argocd/#1-configure-the-applicationyaml","text":"We need to pass an ArgoCD Application spec to the cluster that details how to find and deploy the sandbox services. Copy the Example Bring Your Own Cluster Spec (Argo) and modify as needed. The values look like this, and the spec.source.helm.parameters must be set based on the values gathered in step 0: --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: sandbox-cluster-apps namespace: argocd ## YOUR ARGOCD NAMESPACE spec: destination: server: https://kubernetes.default.svc namespace: argocd ## YOUR ARGOCD NAMESPACE project: default ## YOUR ARGOCD PROJECT source: repoURL: https://github.com/clhain/sandbox-helm-charts.git path: charts/sandbox-apps targetRevision: HEAD helm: parameters: - name: clusterDomain value: # YOUR CLUSTER DOMAIN (e.g. example.com) - name: clusterIngressIP value: # YOUR CLUSTER Ingress IP (e.g. 1.2.3.4) - name: oidcClientID value: # YOUR OIDC Client ID String - name: oidcClientSecret value: # YOUR OIDC Client Secret String - name: oidcIssuerURL value: # YOUR OIDC ISSUER URL (e.g. \"https://login.microsoftonline.com/TENANT/v2.0\") - name: oidcPermittedEmailDomains value: \"*\" # YOUR PERMITTED OIDC EMAIL DOMAINS (e.g. example.com) - name: letsEncryptContactEmail value: # YOUR LETS ENCRYPT CONTACT EMAIL (e.g. nobody@example.com) - name apps.argo-virtual-server.enabled value: false # Turn off the ArgoCD Virtual server assuming it's already exposed via manual process. syncPolicy: automated: prune: true selfHeal: true Additional customization is possible via the helm parameters. See Customizing Default Services for more info.","title":"1. Configure The Application.yaml"},{"location":"installation/byoc-argocd/#2-install-the-sandbox-argocd-application","text":"kubectl apply -f /path/to/your/application.yaml Following that, the individual sandbox components will be installed by ArgoCD over the next 10-20 minutes. You can track the installation progress after connecting to the cluster with: kubectl get application -n argocd Once you see the following, you should be good to go: NAME SYNC STATUS HEALTH STATUS argo-virtual-server Synced Healthy cert-manager Synced Healthy grafana Synced Healthy loki Synced Healthy nginx-ingress Synced Healthy nginx-mesh Synced Healthy oauth-proxy Synced Healthy opentelemetry-operator Synced Healthy prometheus-operator Synced Healthy prometheus-operator-crds Synced Healthy sandbox-apps Synced Healthy temppo Synced Healthy","title":"2. Install The Sandbox ArgoCD Application"},{"location":"installation/byoc-argocd/#3-verification","text":"If you have the argocd CLI utility installed , you can use it to view additional information and troubleshoot issues as needed: 1) Fetch the ArgoCD Admin Password kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo 2) Port Forward ArgoCD CLI Commands export ARGOCD_OPTS='--port-forward-namespace argocd' 3) Login To The ArgoCD Instance argocd login --port-forward --insecure 4) View App Rollout Progress argocd app get sandbox-apps When complete you should see a list that looks like this (all items are Synced and Healthy/blank): GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Secret oauth-proxy dex-config-secret Synced secret/dex-config-secret configured argoproj.io Application argocd cert-manager Synced Healthy application.argoproj.io/cert-manager configured argoproj.io Application argocd nginx-ingress Synced Healthy application.argoproj.io/nginx-ingress configured ConfigMap kube-system coredns Synced Namespace oauth-proxy Synced Secret oauth-proxy oauth-proxy-creds Synced argoproj.io AppProject argocd cluster-services Synced argoproj.io Application argocd argo-virtual-server Synced Healthy argoproj.io Application argocd gatekeeper Synced Healthy argoproj.io Application argocd grafana Synced Healthy argoproj.io Application argocd loki Synced Healthy argoproj.io Application argocd nginx-mesh Synced Healthy argoproj.io Application argocd oauth-proxy Synced Healthy argoproj.io Application argocd opentelemetry-operator Synced Healthy argoproj.io Application argocd prometheus-operator Synced Healthy argoproj.io Application argocd prometheus-operator-crds Synced Healthy argoproj.io Application argocd sealed-secrets Synced Healthy argoproj.io Application argocd tempo Synced Healthy","title":"3. Verification"},{"location":"installation/byoc-argocd/#4-access-sandbox-services","text":"Once all services are in 'Synced, Healthy' state, and you've updated the DNS records as described here , you should be able to securely access the ArgoCD and Grafana services at: https://argocd.YOUR_DOMAIN/ https://grafana.YOUR_DOMAIN/ If you're using the default Dex IDP, you can fetch the credentials for the admin@example.com domain from the cluster as follows: kubectl get secret -n oauth-proxy oauth-proxy-creds -o jsonpath=\"{.data.admin-password}\" | base64 -d; echo Please see the troubleshooting guide for investigating issues.","title":"4. Access Sandbox Services"},{"location":"installation/byoc-helm/","text":"Bring Your Own Cluster (HELM) If you already have a cluster ready to deploy the Sandbox apps set to, you can do so with Helm. If you're installing on a local Kind cluster, check the Quick Start guide for local cluster specific instructions. 0. Pre-Requisites See the guide for Common Sandbox Pre-Reqs for the set of parameters you'll need to have available. In short, they include: clusterDomain: # e.g. yourdomain.com clusterIngressIP: # e.g. 1.2.3.4 oidcClientID: # e.g. some-client-id (optional if using Local Dex) oidcClientSecret: # e.g. some-secret-value (optional if using Local Dex) oidcIssuerURL: # e.g. \"https://login.microsoftonline.com/TENANT/v2.0\" (optional if using Local Dex) oidcPermittedEmailDomains: # e.g. \"yourdomain.com\" (optional if using Local Dex) letsEncryptContactEmail: # e.g. \"someone@yourdomain.com\" 1. Install Helm See the Helm Installation Guide for instructions. 2. Add the Sandbox Chart Repo helm repo add sandbox-charts https://clhain.github.io/sandbox-helm-charts 3. Configure the installation values.yaml We can pass a custom values.yaml file to the helm install command to set the parameters gathered in step 0 above. Copy the Example Bring Your Own Cluster Config and modify as needed. The most common values needed look like this: clusterDomain: # e.g. yourdomain.com clusterIngressIP: # e.g. 1.2.3.4 oidcClientID: # e.g. some-client-id oidcClientSecret: # e.g. some-secret-value oidcIssuerURL: # e.g. \"https://login.microsoftonline.com/TENANT/v2.0\" oidcPermittedEmailDomains: # e.g. \"yourdomain.com\" letsEncryptContactEmail: # e.g. \"someone@yourdomain.com\" You can remove all oidc* values if you're using the included local dex IDP. You can remove letsEncryptContactEmail if you've set clusterTLSEnabled=false. 4. Install The Sandbox Base Helm Chart helm upgrade --install sandbox sandbox-charts/sandbox-base --namespace argocd --values sandbox-values.yaml --wait --atomic --create-namespace Following that, the individual sandbox components will be installed by ArgoCD over the next 10-20 minutes. You can track the installation progress after connecting to the cluster with: kubectl get application -n argocd Once you see the following, you should be good to go (see verification step below): NAME SYNC STATUS HEALTH STATUS argo-virtual-server Synced Healthy cert-manager Synced Healthy grafana Synced Healthy loki Synced Healthy nginx-ingress Synced Healthy nginx-mesh Synced Healthy oauth-proxy Synced Healthy opentelemetry-operator Synced Healthy prometheus-operator Synced Healthy prometheus-operator-crds Synced Healthy sandbox-apps Synced Healthy temppo Synced Healthy 5. Verification If you have the argocd CLI utility installed , you can use it to view additional information and troubleshoot issues as needed: 1) Fetch the ArgoCD Admin Password kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo 2) Port Forward ArgoCD CLI Commands export ARGOCD_OPTS='--port-forward-namespace argocd' 3) Login To The ArgoCD Instance argocd login --port-forward --insecure 4) View App Rollout Progress argocd app get sandbox-apps When complete you should see a list that looks like this (all items are Synced and Healthy/blank): GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Secret oauth-proxy dex-config-secret Synced secret/dex-config-secret configured argoproj.io Application argocd cert-manager Synced Healthy application.argoproj.io/cert-manager configured argoproj.io Application argocd nginx-ingress Synced Healthy application.argoproj.io/nginx-ingress configured ConfigMap kube-system coredns Synced Namespace oauth-proxy Synced Secret oauth-proxy oauth-proxy-creds Synced argoproj.io AppProject argocd cluster-services Synced argoproj.io Application argocd argo-virtual-server Synced Healthy argoproj.io Application argocd gatekeeper Synced Healthy argoproj.io Application argocd grafana Synced Healthy argoproj.io Application argocd loki Synced Healthy argoproj.io Application argocd nginx-mesh Synced Healthy argoproj.io Application argocd oauth-proxy Synced Healthy argoproj.io Application argocd opentelemetry-operator Synced Healthy argoproj.io Application argocd prometheus-operator Synced Healthy argoproj.io Application argocd prometheus-operator-crds Synced Healthy argoproj.io Application argocd sealed-secrets Synced Healthy argoproj.io Application argocd tempo Synced Healthy 6. Access Sandbox Services Once all services are in 'Synced, Healthy' state, and you've updated the DNS records as described here , you should be able to securely access the ArgoCD and Grafana services at: https://argocd.YOUR_DOMAIN/ https://grafana.YOUR_DOMAIN/ If you're using the default Dex IDP, you can fetch the credentials for the admin@example.com domain from the cluster as follows: kubectl get secret -n oauth-proxy oauth-proxy-creds -o jsonpath=\"{.data.admin-password}\" | base64 -d; echo Please see the troubleshooting guide for investigating issues.","title":"Bring Your Own Cluster (Helm)"},{"location":"installation/byoc-helm/#bring-your-own-cluster-helm","text":"If you already have a cluster ready to deploy the Sandbox apps set to, you can do so with Helm. If you're installing on a local Kind cluster, check the Quick Start guide for local cluster specific instructions.","title":"Bring Your Own Cluster (HELM)"},{"location":"installation/byoc-helm/#0-pre-requisites","text":"See the guide for Common Sandbox Pre-Reqs for the set of parameters you'll need to have available. In short, they include: clusterDomain: # e.g. yourdomain.com clusterIngressIP: # e.g. 1.2.3.4 oidcClientID: # e.g. some-client-id (optional if using Local Dex) oidcClientSecret: # e.g. some-secret-value (optional if using Local Dex) oidcIssuerURL: # e.g. \"https://login.microsoftonline.com/TENANT/v2.0\" (optional if using Local Dex) oidcPermittedEmailDomains: # e.g. \"yourdomain.com\" (optional if using Local Dex) letsEncryptContactEmail: # e.g. \"someone@yourdomain.com\"","title":"0. Pre-Requisites"},{"location":"installation/byoc-helm/#1-install-helm","text":"See the Helm Installation Guide for instructions.","title":"1. Install Helm"},{"location":"installation/byoc-helm/#2-add-the-sandbox-chart-repo","text":"helm repo add sandbox-charts https://clhain.github.io/sandbox-helm-charts","title":"2. Add the Sandbox Chart Repo"},{"location":"installation/byoc-helm/#3-configure-the-installation-valuesyaml","text":"We can pass a custom values.yaml file to the helm install command to set the parameters gathered in step 0 above. Copy the Example Bring Your Own Cluster Config and modify as needed. The most common values needed look like this: clusterDomain: # e.g. yourdomain.com clusterIngressIP: # e.g. 1.2.3.4 oidcClientID: # e.g. some-client-id oidcClientSecret: # e.g. some-secret-value oidcIssuerURL: # e.g. \"https://login.microsoftonline.com/TENANT/v2.0\" oidcPermittedEmailDomains: # e.g. \"yourdomain.com\" letsEncryptContactEmail: # e.g. \"someone@yourdomain.com\" You can remove all oidc* values if you're using the included local dex IDP. You can remove letsEncryptContactEmail if you've set clusterTLSEnabled=false.","title":"3. Configure the installation values.yaml"},{"location":"installation/byoc-helm/#4-install-the-sandbox-base-helm-chart","text":"helm upgrade --install sandbox sandbox-charts/sandbox-base --namespace argocd --values sandbox-values.yaml --wait --atomic --create-namespace Following that, the individual sandbox components will be installed by ArgoCD over the next 10-20 minutes. You can track the installation progress after connecting to the cluster with: kubectl get application -n argocd Once you see the following, you should be good to go (see verification step below): NAME SYNC STATUS HEALTH STATUS argo-virtual-server Synced Healthy cert-manager Synced Healthy grafana Synced Healthy loki Synced Healthy nginx-ingress Synced Healthy nginx-mesh Synced Healthy oauth-proxy Synced Healthy opentelemetry-operator Synced Healthy prometheus-operator Synced Healthy prometheus-operator-crds Synced Healthy sandbox-apps Synced Healthy temppo Synced Healthy","title":"4. Install The Sandbox Base Helm Chart"},{"location":"installation/byoc-helm/#5-verification","text":"If you have the argocd CLI utility installed , you can use it to view additional information and troubleshoot issues as needed: 1) Fetch the ArgoCD Admin Password kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo 2) Port Forward ArgoCD CLI Commands export ARGOCD_OPTS='--port-forward-namespace argocd' 3) Login To The ArgoCD Instance argocd login --port-forward --insecure 4) View App Rollout Progress argocd app get sandbox-apps When complete you should see a list that looks like this (all items are Synced and Healthy/blank): GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Secret oauth-proxy dex-config-secret Synced secret/dex-config-secret configured argoproj.io Application argocd cert-manager Synced Healthy application.argoproj.io/cert-manager configured argoproj.io Application argocd nginx-ingress Synced Healthy application.argoproj.io/nginx-ingress configured ConfigMap kube-system coredns Synced Namespace oauth-proxy Synced Secret oauth-proxy oauth-proxy-creds Synced argoproj.io AppProject argocd cluster-services Synced argoproj.io Application argocd argo-virtual-server Synced Healthy argoproj.io Application argocd gatekeeper Synced Healthy argoproj.io Application argocd grafana Synced Healthy argoproj.io Application argocd loki Synced Healthy argoproj.io Application argocd nginx-mesh Synced Healthy argoproj.io Application argocd oauth-proxy Synced Healthy argoproj.io Application argocd opentelemetry-operator Synced Healthy argoproj.io Application argocd prometheus-operator Synced Healthy argoproj.io Application argocd prometheus-operator-crds Synced Healthy argoproj.io Application argocd sealed-secrets Synced Healthy argoproj.io Application argocd tempo Synced Healthy","title":"5. Verification"},{"location":"installation/byoc-helm/#6-access-sandbox-services","text":"Once all services are in 'Synced, Healthy' state, and you've updated the DNS records as described here , you should be able to securely access the ArgoCD and Grafana services at: https://argocd.YOUR_DOMAIN/ https://grafana.YOUR_DOMAIN/ If you're using the default Dex IDP, you can fetch the credentials for the admin@example.com domain from the cluster as follows: kubectl get secret -n oauth-proxy oauth-proxy-creds -o jsonpath=\"{.data.admin-password}\" | base64 -d; echo Please see the troubleshooting guide for investigating issues.","title":"6. Access Sandbox Services"},{"location":"installation/dns/","text":"Sandbox DNS Records In order to direct traffic to the default services enabled by the Sandbox project, DNS records must exist which point the service names to your cluster ingress IP. For ease of management, it's suggested that a single A record be created for the cluster, and then additional CNAME records for the services which point at that cluster A record. Sample Zone File Example records for yourzone.com: ingress IN A <ingress ip address (kubectl get service -n nginx-ingress)> auth IN CNAME ingress.yourzone.com. # Required for oauth2 proxy argocd IN CNAME ingress.yourzone.com. # ArgoCD UI grafana IN CNAME ingress.yourzone.com. # Grafana UI your-app IN CNAME ingress.yourzone.com. # Add any additional services Alternative If you don't have an available DNS domain, you can use a service like nip.io which automatically directs *.[your-ip].nip.io to the ip specified by setting the Cluster Domain to [your-ip].nip.io at install time. This method isn't compatible with Automatic SSL Certs from LetsEncrypt, so you'll also need to either: Disable the TLS endpoints with helm argument --set clusterTLSInsecure=true Modify the cert-manager or virtual-servers to work with alternate settings (see Customizing Default Services )","title":"DNS Configuration"},{"location":"installation/dns/#sandbox-dns-records","text":"In order to direct traffic to the default services enabled by the Sandbox project, DNS records must exist which point the service names to your cluster ingress IP. For ease of management, it's suggested that a single A record be created for the cluster, and then additional CNAME records for the services which point at that cluster A record.","title":"Sandbox DNS Records"},{"location":"installation/dns/#sample-zone-file","text":"Example records for yourzone.com: ingress IN A <ingress ip address (kubectl get service -n nginx-ingress)> auth IN CNAME ingress.yourzone.com. # Required for oauth2 proxy argocd IN CNAME ingress.yourzone.com. # ArgoCD UI grafana IN CNAME ingress.yourzone.com. # Grafana UI your-app IN CNAME ingress.yourzone.com. # Add any additional services","title":"Sample Zone File"},{"location":"installation/dns/#alternative","text":"If you don't have an available DNS domain, you can use a service like nip.io which automatically directs *.[your-ip].nip.io to the ip specified by setting the Cluster Domain to [your-ip].nip.io at install time. This method isn't compatible with Automatic SSL Certs from LetsEncrypt, so you'll also need to either: Disable the TLS endpoints with helm argument --set clusterTLSInsecure=true Modify the cert-manager or virtual-servers to work with alternate settings (see Customizing Default Services )","title":"Alternative"},{"location":"installation/getting-started/","text":"Full Deployment (Choose Install Method) The full featured Sandbox environment can be deployed several ways, depending on your needs. Generally, this is broken down into whether or not you want to have the cluster deployed as part of the installation process, or you already have one available to you. Note: If you're just looking to get a feel for the environment, you can check out the Quick Start guide for deploying to a local Kind cluster (with some features disabled). All-In-One Install The Sandbox project uses Porter based Cloud Native Application Bundles which can combine the deployment of the cluster with the steps to apply the sandbox application set. You can use this method if: You have an account with one of the supported providers Have limited requirements for how the cluster needs to be configured (e.g. node size, location, count, and destination VPC). For more advanced requirements, it's recommended to deploy the cluster manually, and then add the Sandbox apps with one of the Bring Your Own Cluster methods below. Currently, the following infrastructure providers are supported: Google Cloud ( GKE Sandbox Quickstart ) Bring Your Own Cluster If you already have a cluster available, there are 2 ways to layer the Sandbox on top: Helm Based Install (ArgoCD is installed as part of the deployment) ArgoCD Based Install (ArgoCD is already installed on the cluster) In either case, it's strongly recommended that the cluster contain no services (other than ArgoCD for the second method) prior to installation of the Sandbox in order to reduce risk of conflicts. Using Helm There's a \"Sandbox Base\" Helm chart available that will deploy Argo and the full stack of Sandbox Apps to a pre-existing cluster. You should use this method if you have a cluster, but don't have ArgoCD installed on it. See more: Helm Based Install Using ArgoCD If you have a cluster with ArgoCD deployed already, you can install the Sandbox application stack directly via ArgoCD application spec. See more: ArgoCD Based Install","title":"Getting Started"},{"location":"installation/getting-started/#full-deployment-choose-install-method","text":"The full featured Sandbox environment can be deployed several ways, depending on your needs. Generally, this is broken down into whether or not you want to have the cluster deployed as part of the installation process, or you already have one available to you. Note: If you're just looking to get a feel for the environment, you can check out the Quick Start guide for deploying to a local Kind cluster (with some features disabled).","title":"Full Deployment (Choose Install Method)"},{"location":"installation/getting-started/#all-in-one-install","text":"The Sandbox project uses Porter based Cloud Native Application Bundles which can combine the deployment of the cluster with the steps to apply the sandbox application set. You can use this method if: You have an account with one of the supported providers Have limited requirements for how the cluster needs to be configured (e.g. node size, location, count, and destination VPC). For more advanced requirements, it's recommended to deploy the cluster manually, and then add the Sandbox apps with one of the Bring Your Own Cluster methods below. Currently, the following infrastructure providers are supported: Google Cloud ( GKE Sandbox Quickstart )","title":"All-In-One Install"},{"location":"installation/getting-started/#bring-your-own-cluster","text":"If you already have a cluster available, there are 2 ways to layer the Sandbox on top: Helm Based Install (ArgoCD is installed as part of the deployment) ArgoCD Based Install (ArgoCD is already installed on the cluster) In either case, it's strongly recommended that the cluster contain no services (other than ArgoCD for the second method) prior to installation of the Sandbox in order to reduce risk of conflicts.","title":"Bring Your Own Cluster"},{"location":"installation/getting-started/#using-helm","text":"There's a \"Sandbox Base\" Helm chart available that will deploy Argo and the full stack of Sandbox Apps to a pre-existing cluster. You should use this method if you have a cluster, but don't have ArgoCD installed on it. See more: Helm Based Install","title":"Using Helm"},{"location":"installation/getting-started/#using-argocd","text":"If you have a cluster with ArgoCD deployed already, you can install the Sandbox application stack directly via ArgoCD application spec. See more: ArgoCD Based Install","title":"Using ArgoCD"},{"location":"installation/pre-reqs/","text":"Sandbox Pre-Requisites The following things are required to enable all Sandbox functionality. The Quick Start based install (using kind) has no up-front requirements, but some features won't work. The requirements here are common for all fully-featured installations regardless of the install methd. Additional installation specific requirements can be found in the dedicated guides for the All-In-One and Bring Your Own Cluster guides. Cluster Requirements Node Resources The default Sandbox cluster resource utilization at idle is approximately as follows (3 node GKE cluster with e2-standard-2 nodes): Size your nodes taking these baseline performance stats and the specific requirements of your additional apps / services into account. Cluster Ingress IP The Container ingress service requires a static IP address for use with service host records (see DNS section below). Alternatives In order to use the Lets Encrypt certificate issuer, the IP must be publicly accessible for the HTTP verification process. If you don't wish to use this feature, you can either: Disable the TLS endpoints with helm argument --set clusterTLSInsecure=true Modify the cert-manager or virtual-servers to work with alternate settings (see Customizing Default Services ) DNS Zone and Records Access In order to direct external traffic to Sandbox cluster services (and any you'll add yourself), you'll need to have a DNS domain and administrative access to create host records. There are many different services available for this, and we make no specific recommendations. See the DNS Configuration guide for more info. If you're installing the Sandbox via Porter bundle, you'll have to wait until the cluster is created to find the ingress IP assigned. Alternatives If you don't have an available DNS domain, you can use a service like nip.io which automatically directs *.[your-ip].nip.io to the ip specified by setting the Cluster Domain to [your-ip].nip.io at install time. This method isn't compatible with Automatic SSL Certs from LetsEncrypt, so you'll also need to either: Disable the TLS endpoints with helm argument --set clusterTLSInsecure=true Modify the cert-manager or virtual-servers to work with alternate settings (see Customizing Default Services ) Let's Encrypt Contact Email Automated SSL certificate generation via Cert Manager and Let's Encrypt requires an email be provided so they can contact someone regarding problems with the cert, etc. There is no requirement that the email address be associated with a Let's Encrypt account, it can just be any email address you have access to. You don't need to provide this if you're using an alternate Cluster Issuer or have disabled TLS endpoints. OIDC Client Configuration The default sandbox installation comes with Dex and a pre-configured admin account. The process for using an external identity provider is as follows: The Oauth2 Proxy documents has excellent provider-specific instructions for obtaining the needed values and configuring the provider for use. In general, you'll need the following: The OIDC Client ID from the provider. The OIDC Client Secret from the provider. The OIDC Issuer URL for the provider. Authorized Redirects You'll also need to set the following redirect locations as authorized for the client: https://argocd.YOURDOMAIN.COM/auth/callback https://auth.YOURDOMAIN.COM/oauth2/callback See the Oauth2 Proxy doc in this repo for more information.","title":"Common Pre-Reqs"},{"location":"installation/pre-reqs/#sandbox-pre-requisites","text":"The following things are required to enable all Sandbox functionality. The Quick Start based install (using kind) has no up-front requirements, but some features won't work. The requirements here are common for all fully-featured installations regardless of the install methd. Additional installation specific requirements can be found in the dedicated guides for the All-In-One and Bring Your Own Cluster guides.","title":"Sandbox Pre-Requisites"},{"location":"installation/pre-reqs/#cluster-requirements","text":"","title":"Cluster Requirements"},{"location":"installation/pre-reqs/#node-resources","text":"The default Sandbox cluster resource utilization at idle is approximately as follows (3 node GKE cluster with e2-standard-2 nodes): Size your nodes taking these baseline performance stats and the specific requirements of your additional apps / services into account.","title":"Node Resources"},{"location":"installation/pre-reqs/#cluster-ingress-ip","text":"The Container ingress service requires a static IP address for use with service host records (see DNS section below). Alternatives In order to use the Lets Encrypt certificate issuer, the IP must be publicly accessible for the HTTP verification process. If you don't wish to use this feature, you can either: Disable the TLS endpoints with helm argument --set clusterTLSInsecure=true Modify the cert-manager or virtual-servers to work with alternate settings (see Customizing Default Services )","title":"Cluster Ingress IP"},{"location":"installation/pre-reqs/#dns-zone-and-records-access","text":"In order to direct external traffic to Sandbox cluster services (and any you'll add yourself), you'll need to have a DNS domain and administrative access to create host records. There are many different services available for this, and we make no specific recommendations. See the DNS Configuration guide for more info. If you're installing the Sandbox via Porter bundle, you'll have to wait until the cluster is created to find the ingress IP assigned. Alternatives If you don't have an available DNS domain, you can use a service like nip.io which automatically directs *.[your-ip].nip.io to the ip specified by setting the Cluster Domain to [your-ip].nip.io at install time. This method isn't compatible with Automatic SSL Certs from LetsEncrypt, so you'll also need to either: Disable the TLS endpoints with helm argument --set clusterTLSInsecure=true Modify the cert-manager or virtual-servers to work with alternate settings (see Customizing Default Services )","title":"DNS Zone and Records Access"},{"location":"installation/pre-reqs/#lets-encrypt-contact-email","text":"Automated SSL certificate generation via Cert Manager and Let's Encrypt requires an email be provided so they can contact someone regarding problems with the cert, etc. There is no requirement that the email address be associated with a Let's Encrypt account, it can just be any email address you have access to. You don't need to provide this if you're using an alternate Cluster Issuer or have disabled TLS endpoints.","title":"Let's Encrypt Contact Email"},{"location":"installation/pre-reqs/#oidc-client-configuration","text":"The default sandbox installation comes with Dex and a pre-configured admin account. The process for using an external identity provider is as follows: The Oauth2 Proxy documents has excellent provider-specific instructions for obtaining the needed values and configuring the provider for use. In general, you'll need the following: The OIDC Client ID from the provider. The OIDC Client Secret from the provider. The OIDC Issuer URL for the provider.","title":"OIDC Client Configuration"},{"location":"installation/pre-reqs/#authorized-redirects","text":"You'll also need to set the following redirect locations as authorized for the client: https://argocd.YOURDOMAIN.COM/auth/callback https://auth.YOURDOMAIN.COM/oauth2/callback See the Oauth2 Proxy doc in this repo for more information.","title":"Authorized Redirects"},{"location":"installation/quick-start/","text":"Quick Start A local instance of the sandbox can be stood up on a kind cluster to get a feel for it. Some of the components (SSL Auto-Certs for instance) rely on publicly reachable ingress addresses with DNS zones, so consider moving to a cloud provider to take full advantage. Pre-Reqs Install Kind: Kind Quick Start . Install Helm: Helm Install Guide . Install Kubectl: Kubectl Install Guide . Install ArgoCD CLI: Argo Getting Started Guide (Optional) Approximate CPU and Memory utilization on an idle Sandbox cluster can be seen in the Cluster Requirements section of the docs. Installation 1) Add Helm Chart Repo helm repo add sandbox-charts https://clhain.github.io/sandbox-helm-charts 2) Create kind cluster kind create cluster 3) Install Sandbox-Base Helm Chart helm upgrade sandbox-base sandbox-charts/sandbox-base --install --namespace argocd --create-namespace --set clusterTLSInsecure=true Troubleshooting If you see the child apps not progressing, and the following status condition on argocd app get sandbox-apps , it's the result of a race condition where helm applies the sandbox app before the argocd server is ready to handle it. It will eventually retry, but you can speed the process up by forcing a sync immediately with argocd app sync sandbox-apps . CONDITION MESSAGE LAST TRANSITION ComparisonError rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial tcp 10.96.150.135:8081: connect: connection refused\" 2022-10-14 02:55:38 +0000 UTC Verification Once the chart is applied, argocd will start the deployment of the sandbox app suite. This process typically takes 10 or more minutes, and you can view the progress as follows: 1) Fetch the ArgoCD Admin Password kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo 2) Port Forward ArgoCD CLI Commands export ARGOCD_OPTS='--port-forward-namespace argocd' 3) Login To The ArgoCD Instance argocd login --port-forward --insecure 4) View App Rollout Progress argocd app get sandbox-apps When complete you should see a list that looks like this (all items are Synced and Healthy/blank): GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Secret oauth-proxy dex-config-secret Synced secret/dex-config-secret configured argoproj.io Application argocd cert-manager Synced Healthy application.argoproj.io/cert-manager configured argoproj.io Application argocd nginx-ingress Synced Healthy application.argoproj.io/nginx-ingress configured ConfigMap kube-system coredns Synced Namespace oauth-proxy Synced Secret oauth-proxy oauth-proxy-creds Synced argoproj.io AppProject argocd cluster-services Synced argoproj.io Application argocd argo-virtual-server Synced Healthy argoproj.io Application argocd gatekeeper Synced Healthy argoproj.io Application argocd grafana Synced Healthy argoproj.io Application argocd loki Synced Healthy argoproj.io Application argocd nginx-mesh Synced Healthy argoproj.io Application argocd oauth-proxy Synced Healthy argoproj.io Application argocd opentelemetry-operator Synced Healthy argoproj.io Application argocd prometheus-operator Synced Healthy argoproj.io Application argocd prometheus-operator-crds Synced Healthy argoproj.io Application argocd sealed-secrets Synced Healthy argoproj.io Application argocd tempo Synced Healthy Connecting For connecting to the web apps being served, you can use a combination of socat and kubectl to port-forward all port 80 traffic to the nginx-ingress service in the cluster. Run the following commands in different terminals and leave them running. kubectl port-forward -n nginx-ingress service/nginx-ingress-internal 8080:80 Forward Ports sudo socat TCP-LISTEN:80,fork TCP:127.0.0.1:8080 To fetch the login password for the admin@example.com account, run the following: Fetch Admin Account Password kubectl get secret -n oauth-proxy oauth-proxy-creds -o jsonpath=\"{.data.admin-password}\" | base64 -d; echo Login You can login to view the included web interfaces with the admin@example.com username, and password from above. http://grafana.localtest.me http://argocd.localtest.me","title":"Quick Start"},{"location":"installation/quick-start/#quick-start","text":"A local instance of the sandbox can be stood up on a kind cluster to get a feel for it. Some of the components (SSL Auto-Certs for instance) rely on publicly reachable ingress addresses with DNS zones, so consider moving to a cloud provider to take full advantage.","title":"Quick Start"},{"location":"installation/quick-start/#pre-reqs","text":"Install Kind: Kind Quick Start . Install Helm: Helm Install Guide . Install Kubectl: Kubectl Install Guide . Install ArgoCD CLI: Argo Getting Started Guide (Optional) Approximate CPU and Memory utilization on an idle Sandbox cluster can be seen in the Cluster Requirements section of the docs.","title":"Pre-Reqs"},{"location":"installation/quick-start/#installation","text":"1) Add Helm Chart Repo helm repo add sandbox-charts https://clhain.github.io/sandbox-helm-charts 2) Create kind cluster kind create cluster 3) Install Sandbox-Base Helm Chart helm upgrade sandbox-base sandbox-charts/sandbox-base --install --namespace argocd --create-namespace --set clusterTLSInsecure=true","title":"Installation"},{"location":"installation/quick-start/#troubleshooting","text":"If you see the child apps not progressing, and the following status condition on argocd app get sandbox-apps , it's the result of a race condition where helm applies the sandbox app before the argocd server is ready to handle it. It will eventually retry, but you can speed the process up by forcing a sync immediately with argocd app sync sandbox-apps . CONDITION MESSAGE LAST TRANSITION ComparisonError rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial tcp 10.96.150.135:8081: connect: connection refused\" 2022-10-14 02:55:38 +0000 UTC","title":"Troubleshooting"},{"location":"installation/quick-start/#verification","text":"Once the chart is applied, argocd will start the deployment of the sandbox app suite. This process typically takes 10 or more minutes, and you can view the progress as follows: 1) Fetch the ArgoCD Admin Password kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo 2) Port Forward ArgoCD CLI Commands export ARGOCD_OPTS='--port-forward-namespace argocd' 3) Login To The ArgoCD Instance argocd login --port-forward --insecure 4) View App Rollout Progress argocd app get sandbox-apps When complete you should see a list that looks like this (all items are Synced and Healthy/blank): GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Secret oauth-proxy dex-config-secret Synced secret/dex-config-secret configured argoproj.io Application argocd cert-manager Synced Healthy application.argoproj.io/cert-manager configured argoproj.io Application argocd nginx-ingress Synced Healthy application.argoproj.io/nginx-ingress configured ConfigMap kube-system coredns Synced Namespace oauth-proxy Synced Secret oauth-proxy oauth-proxy-creds Synced argoproj.io AppProject argocd cluster-services Synced argoproj.io Application argocd argo-virtual-server Synced Healthy argoproj.io Application argocd gatekeeper Synced Healthy argoproj.io Application argocd grafana Synced Healthy argoproj.io Application argocd loki Synced Healthy argoproj.io Application argocd nginx-mesh Synced Healthy argoproj.io Application argocd oauth-proxy Synced Healthy argoproj.io Application argocd opentelemetry-operator Synced Healthy argoproj.io Application argocd prometheus-operator Synced Healthy argoproj.io Application argocd prometheus-operator-crds Synced Healthy argoproj.io Application argocd sealed-secrets Synced Healthy argoproj.io Application argocd tempo Synced Healthy","title":"Verification"},{"location":"installation/quick-start/#connecting","text":"For connecting to the web apps being served, you can use a combination of socat and kubectl to port-forward all port 80 traffic to the nginx-ingress service in the cluster. Run the following commands in different terminals and leave them running. kubectl port-forward -n nginx-ingress service/nginx-ingress-internal 8080:80 Forward Ports sudo socat TCP-LISTEN:80,fork TCP:127.0.0.1:8080 To fetch the login password for the admin@example.com account, run the following: Fetch Admin Account Password kubectl get secret -n oauth-proxy oauth-proxy-creds -o jsonpath=\"{.data.admin-password}\" | base64 -d; echo Login You can login to view the included web interfaces with the admin@example.com username, and password from above. http://grafana.localtest.me http://argocd.localtest.me","title":"Connecting"},{"location":"services/argocd/","text":"Sandbox Service: OPA Gatekeeper Quick Links Project Site Github Repo Helm Chart Background ArgoCD is a declarative GitOps continuous delivery tool for Kubernetes. It's used extensively in the Sandbox, as the means by which all of the other Sandbox Apps and services are delivered to the cluster. It's also available for Sandbox cluster operators to take advantage of for delivery of their own applications or modifications to the base Sandbox environment. Sandbox Customizations The Sandbox installation of Argocd uses a mostly default set of Helm Values for the official Chart, along with a number of customizations to add resources that deliver and configure the Sandbox Applications. The customizations passed to argo-cd by default are as follows: * Explicitly set the RBAC default Policy to ReadOnly for GUI viewers. * Enables health message checking for theArgoCD application type so ordered deployments (sync waves) will work. It's strongly recommended to include an OIDC config for argoCD. If deploying from the Sandbox-Base Helm chart, or the Porter bundle, the values overrides containing the commented lines below can be added to the sandbox-values file passed in. Modifications can be passed to the official chart (by adding them under a \"argo-cd\" key) via values file as follows: argo-cd: crds: # Required because argo-cd helm handles crds as templates rather than in crds directory. install: false server: # Set the default policy for logged in GUI users to readonly. rbacConfig: policy.default: role:readonly config: resource.customizations.health.argoproj.io_Application: |- hs = {} hs.status = \"Progressing\" hs.message = \"\" if obj.status ~= nil then if obj.status.health ~= nil then hs.status = obj.status.health.status if obj.status.health.message ~= nil then hs.message = obj.status.health.message end end end return hs # Example values for Azure AD # config: # oidc.config: | # name: AzureAD # issuer: {{ YOUR_OIDC_ISSUER_URL }} # clientID: $oauth-secret:oidc.clientId # clientSecret: $oauth-secret:oidc.clientSecret # requestedIDTokenClaims: # groups: # essential: true # requestedScopes: # - openid # - profile # - email See Customizing Default Services for more information on overriding default values.","title":"ArgoCD"},{"location":"services/argocd/#sandbox-service-opa-gatekeeper","text":"","title":"Sandbox Service: OPA Gatekeeper"},{"location":"services/argocd/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/argocd/#background","text":"ArgoCD is a declarative GitOps continuous delivery tool for Kubernetes. It's used extensively in the Sandbox, as the means by which all of the other Sandbox Apps and services are delivered to the cluster. It's also available for Sandbox cluster operators to take advantage of for delivery of their own applications or modifications to the base Sandbox environment.","title":"Background"},{"location":"services/argocd/#sandbox-customizations","text":"The Sandbox installation of Argocd uses a mostly default set of Helm Values for the official Chart, along with a number of customizations to add resources that deliver and configure the Sandbox Applications. The customizations passed to argo-cd by default are as follows: * Explicitly set the RBAC default Policy to ReadOnly for GUI viewers. * Enables health message checking for theArgoCD application type so ordered deployments (sync waves) will work. It's strongly recommended to include an OIDC config for argoCD. If deploying from the Sandbox-Base Helm chart, or the Porter bundle, the values overrides containing the commented lines below can be added to the sandbox-values file passed in. Modifications can be passed to the official chart (by adding them under a \"argo-cd\" key) via values file as follows: argo-cd: crds: # Required because argo-cd helm handles crds as templates rather than in crds directory. install: false server: # Set the default policy for logged in GUI users to readonly. rbacConfig: policy.default: role:readonly config: resource.customizations.health.argoproj.io_Application: |- hs = {} hs.status = \"Progressing\" hs.message = \"\" if obj.status ~= nil then if obj.status.health ~= nil then hs.status = obj.status.health.status if obj.status.health.message ~= nil then hs.message = obj.status.health.message end end end return hs # Example values for Azure AD # config: # oidc.config: | # name: AzureAD # issuer: {{ YOUR_OIDC_ISSUER_URL }} # clientID: $oauth-secret:oidc.clientId # clientSecret: $oauth-secret:oidc.clientSecret # requestedIDTokenClaims: # groups: # essential: true # requestedScopes: # - openid # - profile # - email See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/cert-manager/","text":"Sandbox Service: Cert Manager Quick Links Project Site Github Repo Helm Chart Background Certmanager adds custom kubernetes resources which handle the creation and management of TLS certificates. This functionality is used in several services in the sandbox cluster, including Opentelemetry Operator, NGINX Mesh, and NGINX Ingress. Sandbox Customizations The Sandbox installation of cert-manager includes a mostly default installation with the following modifications: Adds a self-signed Issuer resource in the cert-manager namespace Adds a Let's Encrypt ClusterIssuer resource configured for the Let's Encrypt Staging Environment called letsencrypt-stage Adds a Let's Encrypt ClusterIssuer resource configured for the Let's Encrypt Pord Environment called letsencrypt-prod Configures both Let's Encrypt ClusterIssuers with a required contactEmail address. The issuers can be disabled, and values can be passed to the official cert-manager chart (by adding them under the \"cert-manager\" key), as shown in the last 2 lines of the values file here: enableSelfSignedIssuer: true letsEncryptIssuer: enableProd: true enableStage: true contactEmail: cert-manager: installCRDs: true See Customizing Default Services for more information on overriding default values.","title":"Cert Manager"},{"location":"services/cert-manager/#sandbox-service-cert-manager","text":"","title":"Sandbox Service: Cert Manager"},{"location":"services/cert-manager/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/cert-manager/#background","text":"Certmanager adds custom kubernetes resources which handle the creation and management of TLS certificates. This functionality is used in several services in the sandbox cluster, including Opentelemetry Operator, NGINX Mesh, and NGINX Ingress.","title":"Background"},{"location":"services/cert-manager/#sandbox-customizations","text":"The Sandbox installation of cert-manager includes a mostly default installation with the following modifications: Adds a self-signed Issuer resource in the cert-manager namespace Adds a Let's Encrypt ClusterIssuer resource configured for the Let's Encrypt Staging Environment called letsencrypt-stage Adds a Let's Encrypt ClusterIssuer resource configured for the Let's Encrypt Pord Environment called letsencrypt-prod Configures both Let's Encrypt ClusterIssuers with a required contactEmail address. The issuers can be disabled, and values can be passed to the official cert-manager chart (by adding them under the \"cert-manager\" key), as shown in the last 2 lines of the values file here: enableSelfSignedIssuer: true letsEncryptIssuer: enableProd: true enableStage: true contactEmail: cert-manager: installCRDs: true See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/gatekeeper/","text":"Sandbox Service: OPA Gatekeeper Quick Links Project Site Github Repo Helm Chart Background Open Policy Agent's Gatekeeper project is a Policy Engine for Cloud Native environments. It can act as an admission controller, gating the creation, update, or deletion of K8S resources. Sandbox Customizations The Sandbox installation of Gatekeeper uses the same default values as the official helm chart, with no customizations. Modifications can be passed to the official chart (by adding them under a \"gatekeeper\" key) via values file as follows: gatekeeper: replicas: 3 See Customizing Default Services for more information on overriding default values.","title":"OPA Gatekeeper"},{"location":"services/gatekeeper/#sandbox-service-opa-gatekeeper","text":"","title":"Sandbox Service: OPA Gatekeeper"},{"location":"services/gatekeeper/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/gatekeeper/#background","text":"Open Policy Agent's Gatekeeper project is a Policy Engine for Cloud Native environments. It can act as an admission controller, gating the creation, update, or deletion of K8S resources.","title":"Background"},{"location":"services/gatekeeper/#sandbox-customizations","text":"The Sandbox installation of Gatekeeper uses the same default values as the official helm chart, with no customizations. Modifications can be passed to the official chart (by adding them under a \"gatekeeper\" key) via values file as follows: gatekeeper: replicas: 3 See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/grafana/","text":"Sandbox Service: Grafana Quick Links Project Site Github Repo Helm Chart Background Grafana is the default Observability Platform for the Sandbox. It comes configured out of the box with datasources for Logs, Metrics, and Traces provided via other Sandbox services. It also includes a number of out of the box dashboards which represent the health and state of the cluster as well as other Sandbox Apps (e.g. NGINX ingress, mesh) Sandbox Customizations The Sandbox installation of grafana includes a mostly default installation with the following modifications: Adds a Virtual Server with Oauth2 proxy based authentication. Adds support for automatic provisionning of (viewer) user accounts based on the auth headers passed above. Allows viewers to edit but not save existing dashboards, and use the \"Explore\" tab. Enables sidecar services for automatic dashboard and datasource provisioning. Enables metrics collection by in-cluster prometheus instance via ServiceMonitor. The issuers can be disabled, and values can be passed to the official cert-manager chart (by adding them under the \"grafana\" key), as shown on line 5 of the values file here: enableVirtualServer: true enableOauthRoute: true clusterDomain: example.com grafana: grafana.ini: auth.proxy: enabled: true header_name: X-Auth-Request-Email headers: Email:X-Auth-Request-Email Name:X-Auth-Request-Email users: viewers_can_edit: true sidecar: datasources: enabled: true searchNamespace: ALL dashboards: enabled: true searchNamespace: ALL folderAnnotation: grafana_folder provider: foldersFromFilesStructure: true serviceMonitor: enabled: true See Customizing Default Services for more information on overriding default values. Connecting as Admin You can connect as the Grafana admin user, by fetching the password from the Kubernetes secret and decoding it as follows: kubectl get secret grafana -o=jsonpath='{.data.admin-password}' | base64 -d Adding Dashboards The Sandbox Grafana is configured with a sidecar that automatically detects Dashboard Configmaps in any namespace. To add a new Dashboard, simply add the datasource configuration as a Kubernetes ConfigMap with the label grafana_dashboard: \"1\" . You can also configure the folder using annotations, for example, the following will place the dashboard in a folder named \"Boutique\". annotations: grafana_folder: Boutique Here's a (partial) example dashboard config: apiVersion: v1 kind: ConfigMap metadata: labels: grafana_dashboard: \"1\" name: boutique-dashboard annotations: grafana_folder: Boutique data: boutique-dashboard.json: |- { \"annotations\": { ... Note: If you're deploying with helm, any Grafana variables in the dashboard spec (e.g. {{ .my-variable }}}}), need to be escaped as {{ .my-variable }} Adding Datasources The Sandbox Grafana is configured with a sidecar that automatically detects Datasource Configmaps in any namespace. To add a new datasource, simply add the datasource configuration as a Kubernetes ConfigMap with the label 'grafana_datasource: \"1\"'. Here's an example that deploys a Jaeger Datasource: apiVersion: v1 kind: ConfigMap metadata: name: jaeger-datasource labels: grafana_datasource: \"1\" data: jaeger-datasource.yaml: |- apiVersion: 1 datasources: # This uses the same datasource uid as the disabled tempo source to keep # the link from loki logs -> jaeger working. If jaeger and tempo were both # used for some reason this could be changed, the default loki datasource overriden etc. - uid: XUcrGvZVk orgId: 1 name: Jaeger type: jaeger typeName: Jaeger typeLogoUrl: public/app/plugins/datasource/jaeger/img/jaeger_logo.svg access: proxy url: http://jaeger-query.jaeger.svc:16686 user: '' database: '' basicAuth: false isDefault: false jsonData: tracesToLogs: mapTagNamesEnabled: false readOnly: false","title":"Grafana"},{"location":"services/grafana/#sandbox-service-grafana","text":"","title":"Sandbox Service: Grafana"},{"location":"services/grafana/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/grafana/#background","text":"Grafana is the default Observability Platform for the Sandbox. It comes configured out of the box with datasources for Logs, Metrics, and Traces provided via other Sandbox services. It also includes a number of out of the box dashboards which represent the health and state of the cluster as well as other Sandbox Apps (e.g. NGINX ingress, mesh)","title":"Background"},{"location":"services/grafana/#sandbox-customizations","text":"The Sandbox installation of grafana includes a mostly default installation with the following modifications: Adds a Virtual Server with Oauth2 proxy based authentication. Adds support for automatic provisionning of (viewer) user accounts based on the auth headers passed above. Allows viewers to edit but not save existing dashboards, and use the \"Explore\" tab. Enables sidecar services for automatic dashboard and datasource provisioning. Enables metrics collection by in-cluster prometheus instance via ServiceMonitor. The issuers can be disabled, and values can be passed to the official cert-manager chart (by adding them under the \"grafana\" key), as shown on line 5 of the values file here: enableVirtualServer: true enableOauthRoute: true clusterDomain: example.com grafana: grafana.ini: auth.proxy: enabled: true header_name: X-Auth-Request-Email headers: Email:X-Auth-Request-Email Name:X-Auth-Request-Email users: viewers_can_edit: true sidecar: datasources: enabled: true searchNamespace: ALL dashboards: enabled: true searchNamespace: ALL folderAnnotation: grafana_folder provider: foldersFromFilesStructure: true serviceMonitor: enabled: true See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/grafana/#connecting-as-admin","text":"You can connect as the Grafana admin user, by fetching the password from the Kubernetes secret and decoding it as follows: kubectl get secret grafana -o=jsonpath='{.data.admin-password}' | base64 -d","title":"Connecting as Admin"},{"location":"services/grafana/#adding-dashboards","text":"The Sandbox Grafana is configured with a sidecar that automatically detects Dashboard Configmaps in any namespace. To add a new Dashboard, simply add the datasource configuration as a Kubernetes ConfigMap with the label grafana_dashboard: \"1\" . You can also configure the folder using annotations, for example, the following will place the dashboard in a folder named \"Boutique\". annotations: grafana_folder: Boutique Here's a (partial) example dashboard config: apiVersion: v1 kind: ConfigMap metadata: labels: grafana_dashboard: \"1\" name: boutique-dashboard annotations: grafana_folder: Boutique data: boutique-dashboard.json: |- { \"annotations\": { ... Note: If you're deploying with helm, any Grafana variables in the dashboard spec (e.g. {{ .my-variable }}}}), need to be escaped as {{ .my-variable }}","title":"Adding Dashboards"},{"location":"services/grafana/#adding-datasources","text":"The Sandbox Grafana is configured with a sidecar that automatically detects Datasource Configmaps in any namespace. To add a new datasource, simply add the datasource configuration as a Kubernetes ConfigMap with the label 'grafana_datasource: \"1\"'. Here's an example that deploys a Jaeger Datasource: apiVersion: v1 kind: ConfigMap metadata: name: jaeger-datasource labels: grafana_datasource: \"1\" data: jaeger-datasource.yaml: |- apiVersion: 1 datasources: # This uses the same datasource uid as the disabled tempo source to keep # the link from loki logs -> jaeger working. If jaeger and tempo were both # used for some reason this could be changed, the default loki datasource overriden etc. - uid: XUcrGvZVk orgId: 1 name: Jaeger type: jaeger typeName: Jaeger typeLogoUrl: public/app/plugins/datasource/jaeger/img/jaeger_logo.svg access: proxy url: http://jaeger-query.jaeger.svc:16686 user: '' database: '' basicAuth: false isDefault: false jsonData: tracesToLogs: mapTagNamesEnabled: false readOnly: false","title":"Adding Datasources"},{"location":"services/loki/","text":"Sandbox Service: Loki Quick Links Project Site Github Repo Helm Chart Background Loki (from Grafana), is the default logging utility for the Sandbox. It's integrated automatically with the platform Grafana instance, and can be configured to collect logs for the namespaces you desire. Out of the box, it collects data for the Loki and NGIINX Ingress namespaces. The Sandbox Loki is configured in single-binary mode, and uses filesystem storage. Long term logging retention and resiliency are not supported out of the box. Sandbox Customizations The Sandbox installation of loki includes a mostly default installation with the following modifications: Enables a Grafana Datasource config for automatic integration with the cluster Grafana Enables a PodLogs config which can be configured to automatically log containers in a list of namespaces. Sets the list of namespaces which are logged by default to nginx-ingress. Disables multi-tenancy / loki authentication Sets storage and replication parameters Enables out of the box Loki Grafana Dashboard. Configures logging of the Loki namespace. Values can be passed to the official loki chart (by adding them under the \"loki\" key), as shown on line 6 of the values file here: enableGrafanaDatasource: true enablePodLogs: true podLogsEnabledNamespaces: - nginx-ingress loki: loki: auth_enabled: false commonConfig: replication_factor: 1 storage: type: 'filesystem' monitoring: dashboards: labels: grafana_dashboard: \"1\" annotations: grafana_folder: \"Loki\" selfMonitoring: enabled: true See Customizing Default Services for more information on overriding default values. Enable Logging For A Namespace You can enable logging for a namepace at install time by passing in values for podLogsEnabledNamespaces. You can also add a PodLogs resource(s) along side your own application by including a spec as follows: apiVersion: monitoring.grafana.com/v1alpha1 kind: PodLogs metadata: labels: instance: primary name: kubernetes-pods spec: pipelineStages: - cri: {} namespaceSelector: matchNames: - YOUR NAMESPACE(S) HERE selector: matchLabels: {}","title":"Loki"},{"location":"services/loki/#sandbox-service-loki","text":"","title":"Sandbox Service: Loki"},{"location":"services/loki/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/loki/#background","text":"Loki (from Grafana), is the default logging utility for the Sandbox. It's integrated automatically with the platform Grafana instance, and can be configured to collect logs for the namespaces you desire. Out of the box, it collects data for the Loki and NGIINX Ingress namespaces. The Sandbox Loki is configured in single-binary mode, and uses filesystem storage. Long term logging retention and resiliency are not supported out of the box.","title":"Background"},{"location":"services/loki/#sandbox-customizations","text":"The Sandbox installation of loki includes a mostly default installation with the following modifications: Enables a Grafana Datasource config for automatic integration with the cluster Grafana Enables a PodLogs config which can be configured to automatically log containers in a list of namespaces. Sets the list of namespaces which are logged by default to nginx-ingress. Disables multi-tenancy / loki authentication Sets storage and replication parameters Enables out of the box Loki Grafana Dashboard. Configures logging of the Loki namespace. Values can be passed to the official loki chart (by adding them under the \"loki\" key), as shown on line 6 of the values file here: enableGrafanaDatasource: true enablePodLogs: true podLogsEnabledNamespaces: - nginx-ingress loki: loki: auth_enabled: false commonConfig: replication_factor: 1 storage: type: 'filesystem' monitoring: dashboards: labels: grafana_dashboard: \"1\" annotations: grafana_folder: \"Loki\" selfMonitoring: enabled: true See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/loki/#enable-logging-for-a-namespace","text":"You can enable logging for a namepace at install time by passing in values for podLogsEnabledNamespaces. You can also add a PodLogs resource(s) along side your own application by including a spec as follows: apiVersion: monitoring.grafana.com/v1alpha1 kind: PodLogs metadata: labels: instance: primary name: kubernetes-pods spec: pipelineStages: - cri: {} namespaceSelector: matchNames: - YOUR NAMESPACE(S) HERE selector: matchLabels: {}","title":"Enable Logging For A Namespace"},{"location":"services/nginx-ingress/","text":"Sandbox Service: NGINX Kubernetes Ingress Controller Quick Links Project Site Github Repo Helm Chart Background NGINX Ingress (from NGINX INC), is the default container ingress for the Sandbox platform. It's integrated automatically with the platform Observability tooling (metrics, logs, traces, dashboards), Certificate Manager and Let's Encrypt for automated cert provisioning, and contains reference patterns for integrating with Oauth2 Proxy for authenticated ingress enforcement. The out of the box configs will run with either NGINX Ingress Open Source, or the NGINX Plus based version. Sandbox Customizations The Sandbox installation of nginx ingress includes a mostly default installation with the following modifications: Enables a Grafana Dashboard config for automatic integration with the cluster Grafana Enables Cert Manager integration Enables health status endpoints Enables various parameters like 8k buffer size and http2 for integration with other Sandbox Apps. Enables opentracing and forwards spans to the Sandbox Opentelemetry Collector. Adds custom logging config with extensive performance and other data attributes for each request. Enables prometheus compatible scrape endpoint for metrics. Values can be passed to the official nginx-ingress chart (by adding them under the \"nginx-ingress\" key), as shown on line 4 of the values file here: enableIngressDashboard: true enableAppProtectDashboard: false nginx-ingress: controller: enableCertManager: true enableSnippets: true enablePreviewPolicies: true healthStatus: true nginxplus: false appprotect: enable: false config: name: nginx-config entries: # For oauth2 proxy azure integration proxy-buffer-size: \"8k\" # For GRPC support http2: \"true\" # Opentracing configs opentracing: \"true\" opentracing-tracer: /usr/local/lib/libjaegertracing_plugin.so location-snippets: \"opentracing_propagate_context;\" opentracing-tracer-config: '{\"service_name\": \"nginx-ingress\",\"diabled\": false,\"propagation_format\": \"w3c\",\"reporter\": {\"logSpans\": true,\"endpoint\": \"http://otc-collector.opentelemetry-operator.svc:14268/api/traces\"},\"sampler\": {\"type\": \"const\",\"param\": \"1\"}}' # Additional Logging config / format log-format-escape-json: \"true\" log-format: | {\"timestamp\": \"$time_iso8601\", \"host\": \"$host\", \"uri\": \"$request_uri\", \"upstreamStatus\": \"$upstream_status\", \"upstreamAddr\": \"$upstream_addr\", \"requestMethod\": \"$request_method\", \"requestUrl\": \"$host$request_uri\", \"status\": $status, \"requestSize\": \"$request_length\", \"responseSize\": \"$upstream_response_length\", \"userAgent\": \"$http_user_agent\", \"xff\": \"$http_x_forwarded_for\", \"xfh\": \"$http_x_forwarded_host\", \"xfp\": \"$http_x_forwarded_proto\", \"remoteIp\": \"$remote_addr\", \"referer\": \"$http_referer\", \"latency\": \"$upstream_response_time\", \"protocol\":\"$server_protocol\", \"requestTime\": \"$request_time\", \"upstreamConnectTime\": \"$upstream_connect_time\", \"upstreamHeaderTime\":\"$upstream_header_time\", \"upstreamResponseTime\": \"$upstream_response_time\", \"traceParent\": \"$opentracing_context_traceparent\"} service: annotations: networking.gke.io/internal-load-balancer-allow-global-access: \"true\" extraLabels: app: kic-nginx-ingress prometheus: create: true port: 9113 See Customizing Default Services for more information on overriding default values. Enable A Virtual Server / Ingress for A Service Virtual Servers can be enabled with Oauth2 Proxy integration (or without), through the NGINX Ingress custom resources. This example enables ingress for the Grafana instance and passes authentication information from Oauth2 proxy. In this case, the top level Virtual Server passes traffic to the individual Virtual Server Routes for Grafana and Oauth2 Proxy. Note: the \"{{ .Values.clusterDomain }}\" field is passed to the cluster via helm in a default install. apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: grafana namespace: \"grafana\" spec: host: \"grafana.{{ .Values.clusterDomain }}\" tls: cert-manager: cluster-issuer: letsencrypt-prod secret: grafana-cert routes: - path: / route: grafana/grafana location-snippets: | auth_request /oauth2/auth; error_page 401 = https://auth.{{ .Values.clusterDomain }}/oauth2/start?rd=https://$host$uri; auth_request_set $user $upstream_http_x_auth_request_user; auth_request_set $email $upstream_http_x_auth_request_email; auth_request_set $auth_header $upstream_http_authorization; auth_request_set $token $upstream_http_x_auth_request_access_token; proxy_set_header X-Access-Token $token; auth_request_set $auth_cookie $upstream_http_set_cookie; add_header Set-Cookie $auth_cookie; proxy_set_header X-Auth-Request-Email \"$email\"; proxy_set_header X-Auth-Request-User \"$user\"; #proxy_set_header Authorization \"$auth_header\"; set $session_id_header $upstream_http_x_auth_request_email; - path: /oauth2 route: oauth-proxy/oauth-proxy-grafana --- apiVersion: k8s.nginx.org/v1 kind: VirtualServerRoute metadata: name: grafana namespace: grafana spec: host: \"grafana.{{ .Values.clusterDomain }}\" upstreams: - name: grafana service: grafana port: 80 subroutes: - path: / action: pass: grafana --- apiVersion: k8s.nginx.org/v1 kind: VirtualServerRoute metadata: name: oauth-proxy-grafana namespace: oauth-proxy spec: host: \"grafana.{{ .Values.clusterDomain }}\" upstreams: - name: oauth2-proxy service: oauth-proxy-oauth2-proxy port: 80 subroutes: - path: /oauth2/auth location-snippets: \"internal;\" action: pass: oauth2-proxy","title":"NGINX Kubernetes Ingress"},{"location":"services/nginx-ingress/#sandbox-service-nginx-kubernetes-ingress-controller","text":"","title":"Sandbox Service: NGINX Kubernetes Ingress Controller"},{"location":"services/nginx-ingress/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/nginx-ingress/#background","text":"NGINX Ingress (from NGINX INC), is the default container ingress for the Sandbox platform. It's integrated automatically with the platform Observability tooling (metrics, logs, traces, dashboards), Certificate Manager and Let's Encrypt for automated cert provisioning, and contains reference patterns for integrating with Oauth2 Proxy for authenticated ingress enforcement. The out of the box configs will run with either NGINX Ingress Open Source, or the NGINX Plus based version.","title":"Background"},{"location":"services/nginx-ingress/#sandbox-customizations","text":"The Sandbox installation of nginx ingress includes a mostly default installation with the following modifications: Enables a Grafana Dashboard config for automatic integration with the cluster Grafana Enables Cert Manager integration Enables health status endpoints Enables various parameters like 8k buffer size and http2 for integration with other Sandbox Apps. Enables opentracing and forwards spans to the Sandbox Opentelemetry Collector. Adds custom logging config with extensive performance and other data attributes for each request. Enables prometheus compatible scrape endpoint for metrics. Values can be passed to the official nginx-ingress chart (by adding them under the \"nginx-ingress\" key), as shown on line 4 of the values file here: enableIngressDashboard: true enableAppProtectDashboard: false nginx-ingress: controller: enableCertManager: true enableSnippets: true enablePreviewPolicies: true healthStatus: true nginxplus: false appprotect: enable: false config: name: nginx-config entries: # For oauth2 proxy azure integration proxy-buffer-size: \"8k\" # For GRPC support http2: \"true\" # Opentracing configs opentracing: \"true\" opentracing-tracer: /usr/local/lib/libjaegertracing_plugin.so location-snippets: \"opentracing_propagate_context;\" opentracing-tracer-config: '{\"service_name\": \"nginx-ingress\",\"diabled\": false,\"propagation_format\": \"w3c\",\"reporter\": {\"logSpans\": true,\"endpoint\": \"http://otc-collector.opentelemetry-operator.svc:14268/api/traces\"},\"sampler\": {\"type\": \"const\",\"param\": \"1\"}}' # Additional Logging config / format log-format-escape-json: \"true\" log-format: | {\"timestamp\": \"$time_iso8601\", \"host\": \"$host\", \"uri\": \"$request_uri\", \"upstreamStatus\": \"$upstream_status\", \"upstreamAddr\": \"$upstream_addr\", \"requestMethod\": \"$request_method\", \"requestUrl\": \"$host$request_uri\", \"status\": $status, \"requestSize\": \"$request_length\", \"responseSize\": \"$upstream_response_length\", \"userAgent\": \"$http_user_agent\", \"xff\": \"$http_x_forwarded_for\", \"xfh\": \"$http_x_forwarded_host\", \"xfp\": \"$http_x_forwarded_proto\", \"remoteIp\": \"$remote_addr\", \"referer\": \"$http_referer\", \"latency\": \"$upstream_response_time\", \"protocol\":\"$server_protocol\", \"requestTime\": \"$request_time\", \"upstreamConnectTime\": \"$upstream_connect_time\", \"upstreamHeaderTime\":\"$upstream_header_time\", \"upstreamResponseTime\": \"$upstream_response_time\", \"traceParent\": \"$opentracing_context_traceparent\"} service: annotations: networking.gke.io/internal-load-balancer-allow-global-access: \"true\" extraLabels: app: kic-nginx-ingress prometheus: create: true port: 9113 See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/nginx-ingress/#enable-a-virtual-server-ingress-for-a-service","text":"Virtual Servers can be enabled with Oauth2 Proxy integration (or without), through the NGINX Ingress custom resources. This example enables ingress for the Grafana instance and passes authentication information from Oauth2 proxy. In this case, the top level Virtual Server passes traffic to the individual Virtual Server Routes for Grafana and Oauth2 Proxy. Note: the \"{{ .Values.clusterDomain }}\" field is passed to the cluster via helm in a default install. apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: grafana namespace: \"grafana\" spec: host: \"grafana.{{ .Values.clusterDomain }}\" tls: cert-manager: cluster-issuer: letsencrypt-prod secret: grafana-cert routes: - path: / route: grafana/grafana location-snippets: | auth_request /oauth2/auth; error_page 401 = https://auth.{{ .Values.clusterDomain }}/oauth2/start?rd=https://$host$uri; auth_request_set $user $upstream_http_x_auth_request_user; auth_request_set $email $upstream_http_x_auth_request_email; auth_request_set $auth_header $upstream_http_authorization; auth_request_set $token $upstream_http_x_auth_request_access_token; proxy_set_header X-Access-Token $token; auth_request_set $auth_cookie $upstream_http_set_cookie; add_header Set-Cookie $auth_cookie; proxy_set_header X-Auth-Request-Email \"$email\"; proxy_set_header X-Auth-Request-User \"$user\"; #proxy_set_header Authorization \"$auth_header\"; set $session_id_header $upstream_http_x_auth_request_email; - path: /oauth2 route: oauth-proxy/oauth-proxy-grafana --- apiVersion: k8s.nginx.org/v1 kind: VirtualServerRoute metadata: name: grafana namespace: grafana spec: host: \"grafana.{{ .Values.clusterDomain }}\" upstreams: - name: grafana service: grafana port: 80 subroutes: - path: / action: pass: grafana --- apiVersion: k8s.nginx.org/v1 kind: VirtualServerRoute metadata: name: oauth-proxy-grafana namespace: oauth-proxy spec: host: \"grafana.{{ .Values.clusterDomain }}\" upstreams: - name: oauth2-proxy service: oauth-proxy-oauth2-proxy port: 80 subroutes: - path: /oauth2/auth location-snippets: \"internal;\" action: pass: oauth2-proxy","title":"Enable A Virtual Server / Ingress for A Service"},{"location":"services/nginx-mesh/","text":"Sandbox Service: NGINX Kubernetes Ingress Controller Quick Links Project Site Github Repo Helm Chart Background NGINX Service Mesh is the default service mesh for the Sandbox platform. It's integrated automatically with the platform Observability tooling (metrics, logs, traces, dashboards). Traffic enforcement and automatic sidecar injection are disabled by default, but can be enabled at install time or via per-namespace configs applied with your application (see below). Sandbox Customizations The Sandbox installation of nginx mesh includes a mostly default installation with the following modifications: Enables a Grafana Dashboard config for automatic integration with the cluster Grafana. Disables automatic sidecar injection. Enables opentelemetry for trace export. Disables MTLS Persistent storage mode. Values can be passed to the official nginx-ingress chart (by adding them under the \"nginx-ingress\" key), as shown on line 3 of the values file here: enableMeshDashboards: true nginx-service-mesh: autoInjection: disable: true telemetry: samplerRatio: 1 exporters: otlp: host: \"otc-collector.opentelemetry-operator.svc\" port: 4317 mtls: persistentStorage: \"off\" See Customizing Default Services for more information on overriding default values. Enable A Pod for Mesh Sidecar Injection Annotations can be added to resources to enable / disable sidecar injection. The default Sandbox deployment disables auto-injection. To enable, add the following annotation to the resources PodTemplateSpec. annotations: injector.nsm.nginx.com/auto-inject: \"true\"","title":"NGINX Service Mesh"},{"location":"services/nginx-mesh/#sandbox-service-nginx-kubernetes-ingress-controller","text":"","title":"Sandbox Service: NGINX Kubernetes Ingress Controller"},{"location":"services/nginx-mesh/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/nginx-mesh/#background","text":"NGINX Service Mesh is the default service mesh for the Sandbox platform. It's integrated automatically with the platform Observability tooling (metrics, logs, traces, dashboards). Traffic enforcement and automatic sidecar injection are disabled by default, but can be enabled at install time or via per-namespace configs applied with your application (see below).","title":"Background"},{"location":"services/nginx-mesh/#sandbox-customizations","text":"The Sandbox installation of nginx mesh includes a mostly default installation with the following modifications: Enables a Grafana Dashboard config for automatic integration with the cluster Grafana. Disables automatic sidecar injection. Enables opentelemetry for trace export. Disables MTLS Persistent storage mode. Values can be passed to the official nginx-ingress chart (by adding them under the \"nginx-ingress\" key), as shown on line 3 of the values file here: enableMeshDashboards: true nginx-service-mesh: autoInjection: disable: true telemetry: samplerRatio: 1 exporters: otlp: host: \"otc-collector.opentelemetry-operator.svc\" port: 4317 mtls: persistentStorage: \"off\" See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/nginx-mesh/#enable-a-pod-for-mesh-sidecar-injection","text":"Annotations can be added to resources to enable / disable sidecar injection. The default Sandbox deployment disables auto-injection. To enable, add the following annotation to the resources PodTemplateSpec. annotations: injector.nsm.nginx.com/auto-inject: \"true\"","title":"Enable A Pod for Mesh Sidecar Injection"},{"location":"services/oauth-proxy/","text":"Sandbox Service: Oauth2 Proxy Quick Links Project Site Github Repo Helm Chart Background The Oauth2 Proxy service is integrated with NGINX Ingress to provide authentication capabilities for services running in the cluster. The Oauth2 Proxy can be configured to integrate with a variety of different Identity Providers via ODIC. Sandbox Customizations The Sandbox installation of Oauth2 Proxy includes a mostly default installation with the following modifications: Enables a virtual server for the cluster service at \"auth. \" Leverages an existing secret for OIDC client data (deployed as part of the sandbox-apps proxy helm chart). Enables authorization / token / user headers to upstream services. Values can be passed to the official oauth2-proxy chart (by adding them under the \"oauth2-proxy\" key), as shown on line 4 of the values file here: clusterDomain: example.com enableVirtualServer: true oauth2-proxy: config: existingSecret: oauth-proxy-creds configFile: |- upstreams=[ \"file:///dev/null\" ] provider=\"oidc\" http_address=\"0.0.0.0:4180\" set_xauthrequest=true cookie_secure=true skip_jwt_bearer_tokens=true pass_access_token=true pass_authorization_header=true pass_user_headers=true set_authorization_header=true See Customizing Default Services for more information on overriding default values. Enable A Virtual Server For Oauth Proxy Authentication The following Grafana virtual server config illustrates the integration of NGINX ingress and Oauth2 Proxy to authenticate users accessing services in the cluster. The Virtual Server directs traffic to either the Grafana or Ouath2 Proxy Virtual Server Routes, depending on the state of the client. Note: the \"{{ .Values.clusterDomain }}\" field is passed to the cluster via helm in a default install. apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: grafana namespace: \"grafana\" spec: host: \"grafana.{{ .Values.clusterDomain }}\" tls: cert-manager: cluster-issuer: letsencrypt-prod secret: grafana-cert routes: - path: / route: grafana/grafana location-snippets: | auth_request /oauth2/auth; error_page 401 = https://auth.{{ .Values.clusterDomain }}/oauth2/start?rd=https://$host$uri; auth_request_set $user $upstream_http_x_auth_request_user; auth_request_set $email $upstream_http_x_auth_request_email; auth_request_set $auth_header $upstream_http_authorization; auth_request_set $token $upstream_http_x_auth_request_access_token; proxy_set_header X-Access-Token $token; auth_request_set $auth_cookie $upstream_http_set_cookie; add_header Set-Cookie $auth_cookie; proxy_set_header X-Auth-Request-Email \"$email\"; proxy_set_header X-Auth-Request-User \"$user\"; #proxy_set_header Authorization \"$auth_header\"; set $session_id_header $upstream_http_x_auth_request_email; - path: /oauth2 route: oauth-proxy/oauth-proxy-grafana --- apiVersion: k8s.nginx.org/v1 kind: VirtualServerRoute metadata: name: grafana namespace: grafana spec: host: \"grafana.{{ .Values.clusterDomain }}\" upstreams: - name: grafana service: grafana port: 80 subroutes: - path: / action: pass: grafana --- apiVersion: k8s.nginx.org/v1 kind: VirtualServerRoute metadata: name: oauth-proxy-grafana namespace: oauth-proxy spec: host: \"grafana.{{ .Values.clusterDomain }}\" upstreams: - name: oauth2-proxy service: oauth-proxy-oauth2-proxy port: 80 subroutes: - path: /oauth2/auth location-snippets: \"internal;\" action: pass: oauth2-proxy","title":"Oauth2 Proxy"},{"location":"services/oauth-proxy/#sandbox-service-oauth2-proxy","text":"","title":"Sandbox Service: Oauth2 Proxy"},{"location":"services/oauth-proxy/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/oauth-proxy/#background","text":"The Oauth2 Proxy service is integrated with NGINX Ingress to provide authentication capabilities for services running in the cluster. The Oauth2 Proxy can be configured to integrate with a variety of different Identity Providers via ODIC.","title":"Background"},{"location":"services/oauth-proxy/#sandbox-customizations","text":"The Sandbox installation of Oauth2 Proxy includes a mostly default installation with the following modifications: Enables a virtual server for the cluster service at \"auth. \" Leverages an existing secret for OIDC client data (deployed as part of the sandbox-apps proxy helm chart). Enables authorization / token / user headers to upstream services. Values can be passed to the official oauth2-proxy chart (by adding them under the \"oauth2-proxy\" key), as shown on line 4 of the values file here: clusterDomain: example.com enableVirtualServer: true oauth2-proxy: config: existingSecret: oauth-proxy-creds configFile: |- upstreams=[ \"file:///dev/null\" ] provider=\"oidc\" http_address=\"0.0.0.0:4180\" set_xauthrequest=true cookie_secure=true skip_jwt_bearer_tokens=true pass_access_token=true pass_authorization_header=true pass_user_headers=true set_authorization_header=true See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/oauth-proxy/#enable-a-virtual-server-for-oauth-proxy-authentication","text":"The following Grafana virtual server config illustrates the integration of NGINX ingress and Oauth2 Proxy to authenticate users accessing services in the cluster. The Virtual Server directs traffic to either the Grafana or Ouath2 Proxy Virtual Server Routes, depending on the state of the client. Note: the \"{{ .Values.clusterDomain }}\" field is passed to the cluster via helm in a default install. apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: grafana namespace: \"grafana\" spec: host: \"grafana.{{ .Values.clusterDomain }}\" tls: cert-manager: cluster-issuer: letsencrypt-prod secret: grafana-cert routes: - path: / route: grafana/grafana location-snippets: | auth_request /oauth2/auth; error_page 401 = https://auth.{{ .Values.clusterDomain }}/oauth2/start?rd=https://$host$uri; auth_request_set $user $upstream_http_x_auth_request_user; auth_request_set $email $upstream_http_x_auth_request_email; auth_request_set $auth_header $upstream_http_authorization; auth_request_set $token $upstream_http_x_auth_request_access_token; proxy_set_header X-Access-Token $token; auth_request_set $auth_cookie $upstream_http_set_cookie; add_header Set-Cookie $auth_cookie; proxy_set_header X-Auth-Request-Email \"$email\"; proxy_set_header X-Auth-Request-User \"$user\"; #proxy_set_header Authorization \"$auth_header\"; set $session_id_header $upstream_http_x_auth_request_email; - path: /oauth2 route: oauth-proxy/oauth-proxy-grafana --- apiVersion: k8s.nginx.org/v1 kind: VirtualServerRoute metadata: name: grafana namespace: grafana spec: host: \"grafana.{{ .Values.clusterDomain }}\" upstreams: - name: grafana service: grafana port: 80 subroutes: - path: / action: pass: grafana --- apiVersion: k8s.nginx.org/v1 kind: VirtualServerRoute metadata: name: oauth-proxy-grafana namespace: oauth-proxy spec: host: \"grafana.{{ .Values.clusterDomain }}\" upstreams: - name: oauth2-proxy service: oauth-proxy-oauth2-proxy port: 80 subroutes: - path: /oauth2/auth location-snippets: \"internal;\" action: pass: oauth2-proxy","title":"Enable A Virtual Server For Oauth Proxy Authentication"},{"location":"services/opentelemetry-operator/","text":"Sandbox Service: Opentelemtry Operator Quick Links Project Site Github Repo Helm Chart Background Opentelemetry Operator is used to deploy Opentelemetry Collectors, which act as a central point of collection for platform telemetry. In the default sandbox config, all applicable services are configured to send distributed trace data to the Opentelemetry Collector, which is the forwarded to backend storage (Grafana Tempo). The default installation includes listeners for OTLP, Zipkin, Jaeger. Any OTLP based metrics are forwarded to the cluster Prometheus instance. Sandbox Customizations The Sandbox installation of Oauth2 Proxy includes a mostly default installation with the following modifications: Enables a Grafana dashboard for Collector Metrics visualization. Enables a ServiceMonitor for automated Prometheus metrics collection. Enables RBAC policies necessary for the K8S Attributes processor. Enables a default collector (with config shown below) in the opentelemetry-operator namespace. Values can be passed to the official opentelemetry-operator chart (by adding them under the \"opentelemetry-operator\" key), as shown on line 6 of the values file here: enableDashboard: true enableServiceMonitor: true enableRBAC: true enableDeployment: true opentelemetry-operator: manager: serviceMonitor: enabled: true See Customizing Default Services for more information on overriding default values. Default Collector Config The default Opentelemetry Collector instance is configured as follows: receivers: otlp: protocols: grpc: http: zipkin: endpoint: 0.0.0.0:9411 jaeger: protocols: thrift_http: endpoint: 0.0.0.0:14268 processors: batch: send_batch_size: 10000 timeout: 10s k8sattributes: passthrough: false extract: metadata: - k8s.pod.name - k8s.namespace.name pod_association: - from: resource_attribute name: k8s.pod.ip - from: connection exporters: logging: logging/debug: loglevel: debug otlp/tempo: endpoint: tempo.tempo.svc.cluster.local:4317 tls: insecure: true prometheusremotewrite: endpoint: \"http://prometheus-operator-kube-p-prometheus.prometheus-operator.svc:9090/api/v1/write\" service: telemetry: metrics: level: detailed address: 0.0.0.0:8888 pipelines: traces: receivers: [jaeger, zipkin, otlp] processors: [k8sattributes, batch] exporters: [otlp/tempo, logging] metrics: receivers: [otlp] processors: [] exporters: [logging, prometheusremotewrite] Customizing Collector Configs Collector config customization can be done in 2 ways: Disable the default collector and add your own OpentelemetryCollector Resource spec (not recommended). Add an additional Opentelemetry Collector dedicated to your specific needs, and leave the default one as-is. Send Trace / OTLP data to the Collector The default sandbox install includes OTLP (metrics + traces) / Jaeger / Zipkin endpoints, all accessible at otc-collector.opentelemetry-operator.svc. Configure your telemetry sources to send data to the associated ports at that endpoint for processing in accordance with the collector config above.","title":"Opentelemetry Operator"},{"location":"services/opentelemetry-operator/#sandbox-service-opentelemtry-operator","text":"","title":"Sandbox Service: Opentelemtry Operator"},{"location":"services/opentelemetry-operator/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/opentelemetry-operator/#background","text":"Opentelemetry Operator is used to deploy Opentelemetry Collectors, which act as a central point of collection for platform telemetry. In the default sandbox config, all applicable services are configured to send distributed trace data to the Opentelemetry Collector, which is the forwarded to backend storage (Grafana Tempo). The default installation includes listeners for OTLP, Zipkin, Jaeger. Any OTLP based metrics are forwarded to the cluster Prometheus instance.","title":"Background"},{"location":"services/opentelemetry-operator/#sandbox-customizations","text":"The Sandbox installation of Oauth2 Proxy includes a mostly default installation with the following modifications: Enables a Grafana dashboard for Collector Metrics visualization. Enables a ServiceMonitor for automated Prometheus metrics collection. Enables RBAC policies necessary for the K8S Attributes processor. Enables a default collector (with config shown below) in the opentelemetry-operator namespace. Values can be passed to the official opentelemetry-operator chart (by adding them under the \"opentelemetry-operator\" key), as shown on line 6 of the values file here: enableDashboard: true enableServiceMonitor: true enableRBAC: true enableDeployment: true opentelemetry-operator: manager: serviceMonitor: enabled: true See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/opentelemetry-operator/#default-collector-config","text":"The default Opentelemetry Collector instance is configured as follows: receivers: otlp: protocols: grpc: http: zipkin: endpoint: 0.0.0.0:9411 jaeger: protocols: thrift_http: endpoint: 0.0.0.0:14268 processors: batch: send_batch_size: 10000 timeout: 10s k8sattributes: passthrough: false extract: metadata: - k8s.pod.name - k8s.namespace.name pod_association: - from: resource_attribute name: k8s.pod.ip - from: connection exporters: logging: logging/debug: loglevel: debug otlp/tempo: endpoint: tempo.tempo.svc.cluster.local:4317 tls: insecure: true prometheusremotewrite: endpoint: \"http://prometheus-operator-kube-p-prometheus.prometheus-operator.svc:9090/api/v1/write\" service: telemetry: metrics: level: detailed address: 0.0.0.0:8888 pipelines: traces: receivers: [jaeger, zipkin, otlp] processors: [k8sattributes, batch] exporters: [otlp/tempo, logging] metrics: receivers: [otlp] processors: [] exporters: [logging, prometheusremotewrite]","title":"Default Collector Config"},{"location":"services/opentelemetry-operator/#customizing-collector-configs","text":"Collector config customization can be done in 2 ways: Disable the default collector and add your own OpentelemetryCollector Resource spec (not recommended). Add an additional Opentelemetry Collector dedicated to your specific needs, and leave the default one as-is.","title":"Customizing Collector Configs"},{"location":"services/opentelemetry-operator/#send-trace-otlp-data-to-the-collector","text":"The default sandbox install includes OTLP (metrics + traces) / Jaeger / Zipkin endpoints, all accessible at otc-collector.opentelemetry-operator.svc. Configure your telemetry sources to send data to the associated ports at that endpoint for processing in accordance with the collector config above.","title":"Send Trace / OTLP data to the Collector"},{"location":"services/prometheus-operator/","text":"Sandbox Service: Prometheus Operator Quick Links Project Site Github Repo Helm Chart Background Prometheus is the default Metrics collection, storage, and query engine for the Sandbox Platform. It's deeply integrated with Kubernetes, and monitors all aspects of the cluster and most Sandbox Apps automatically. It has repeatable patterns for integrating metrics collection for your own applications, and is automatically configured as a datasource for the Sandbox Grafana instance. Sandbox Customizations The Sandbox installation of prometheus Operator includes a mostly default installation with the following modifications: Prometheus adapter is included to allow Kubernetes to leverage custom metrics (nginx based ones by default) for autoscaling. Grafana provisioning is disabled (handled through a dedicated Grafana chart). Datasource and dashboard deployments are forced despite Grafana being disabled. Remote Write Receiver is enabled (to allow remote write from Opentelemetry Collector). Custom scrape jobs are included to ingest mesh and ingress data. Values can be passed to the official prometheus-adapter chart (by adding them under the \"prometheus-adapter:\" key), as shown on line 1 of the values file. Values can be passed to the official kube-prometheus-stack chart (by adding them under the \"kube-prometheus-stack:\" key), as shown on line 4 of the values file prometheus-adapter: ... kube-prometheus-stack: grafana: enabled: false forceDeployDatasources: true forceDeployDashboards: true sidecar: dashboards: annotations: grafana_folder: Kubernetes prometheus: prometheusSpec: serviceMonitorSelectorNilUsesHelmValues: false enableRemoteWriteReceiver: true additionalScrapeConfigs: - job_name: 'nginx-mesh-sidecars' ... See Customizing Default Services for more information on overriding default values. Enable Metrics Collection For A Service / Pod Metrics collection can be enabled for your service using ServiceMonitor and PodMonitor spec. Here is the servicemonitor definition deployed by the Opentelemetry Operator helm chart. apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: otc namespace: opentelemetry-operator spec: endpoints: - port: monitoring namespaceSelector: matchNames: - opentelemetry-operator selector: matchLabels: app.kubernetes.io/name: otc-collector-monitoring","title":"Prometheus Operator"},{"location":"services/prometheus-operator/#sandbox-service-prometheus-operator","text":"","title":"Sandbox Service: Prometheus Operator"},{"location":"services/prometheus-operator/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/prometheus-operator/#background","text":"Prometheus is the default Metrics collection, storage, and query engine for the Sandbox Platform. It's deeply integrated with Kubernetes, and monitors all aspects of the cluster and most Sandbox Apps automatically. It has repeatable patterns for integrating metrics collection for your own applications, and is automatically configured as a datasource for the Sandbox Grafana instance.","title":"Background"},{"location":"services/prometheus-operator/#sandbox-customizations","text":"The Sandbox installation of prometheus Operator includes a mostly default installation with the following modifications: Prometheus adapter is included to allow Kubernetes to leverage custom metrics (nginx based ones by default) for autoscaling. Grafana provisioning is disabled (handled through a dedicated Grafana chart). Datasource and dashboard deployments are forced despite Grafana being disabled. Remote Write Receiver is enabled (to allow remote write from Opentelemetry Collector). Custom scrape jobs are included to ingest mesh and ingress data. Values can be passed to the official prometheus-adapter chart (by adding them under the \"prometheus-adapter:\" key), as shown on line 1 of the values file. Values can be passed to the official kube-prometheus-stack chart (by adding them under the \"kube-prometheus-stack:\" key), as shown on line 4 of the values file prometheus-adapter: ... kube-prometheus-stack: grafana: enabled: false forceDeployDatasources: true forceDeployDashboards: true sidecar: dashboards: annotations: grafana_folder: Kubernetes prometheus: prometheusSpec: serviceMonitorSelectorNilUsesHelmValues: false enableRemoteWriteReceiver: true additionalScrapeConfigs: - job_name: 'nginx-mesh-sidecars' ... See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/prometheus-operator/#enable-metrics-collection-for-a-service-pod","text":"Metrics collection can be enabled for your service using ServiceMonitor and PodMonitor spec. Here is the servicemonitor definition deployed by the Opentelemetry Operator helm chart. apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: otc namespace: opentelemetry-operator spec: endpoints: - port: monitoring namespaceSelector: matchNames: - opentelemetry-operator selector: matchLabels: app.kubernetes.io/name: otc-collector-monitoring","title":"Enable Metrics Collection For A Service / Pod"},{"location":"services/sealed-secrets/","text":"Sandbox Service: Sealed Secrets Quick Links Project Site Github Repo Helm Chart Background Sealed Secrets from Bitnami Labs is used for encrypting secret values for use in the cluster, so they can be securely stored in systems like a Git repository for use with ArgoCD or other deployment workflows. The Sealed secrets service is installed in the cluster along with the other sandbox apps, and then a command line utility can be used to encrypt secrets for use with that cluster and only that cluster (or those with the private key). Sandbox Customizations The Sandbox installation of prometheus Operator includes a mostly default installation with the following modifications: s * Deploy Grafana dashboard and move it to a folder named \"Sealed Secrets\" * Enable the service monitor for metrics collection Values can be passed to the official sealed-secrets chart (by adding them under the \"sealed-secrets:\" key), as shown on line 1 of the values file below. sealed-secrets: commonAnnotations: grafana_folder: \"Sealed Secrets\" metrics: serviceMonitor: enabled: true dashboards: create: true labels: grafana_dashboard: \"1\" See Customizing Default Services for more information on overriding default values. Seal A Secret Install the kubeseal binary Follow The Usage Directions To Secure a Secret Note: Default helm install requires additional command line arguments to the kubeseal command as follows: kubeseal --controller-name sealed-secrets --controller-namespace sealed-secrets < /tmp/secret.json > sealed-secret.json","title":"Sealed Secrets"},{"location":"services/sealed-secrets/#sandbox-service-sealed-secrets","text":"","title":"Sandbox Service: Sealed Secrets"},{"location":"services/sealed-secrets/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/sealed-secrets/#background","text":"Sealed Secrets from Bitnami Labs is used for encrypting secret values for use in the cluster, so they can be securely stored in systems like a Git repository for use with ArgoCD or other deployment workflows. The Sealed secrets service is installed in the cluster along with the other sandbox apps, and then a command line utility can be used to encrypt secrets for use with that cluster and only that cluster (or those with the private key).","title":"Background"},{"location":"services/sealed-secrets/#sandbox-customizations","text":"The Sandbox installation of prometheus Operator includes a mostly default installation with the following modifications: s * Deploy Grafana dashboard and move it to a folder named \"Sealed Secrets\" * Enable the service monitor for metrics collection Values can be passed to the official sealed-secrets chart (by adding them under the \"sealed-secrets:\" key), as shown on line 1 of the values file below. sealed-secrets: commonAnnotations: grafana_folder: \"Sealed Secrets\" metrics: serviceMonitor: enabled: true dashboards: create: true labels: grafana_dashboard: \"1\" See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"},{"location":"services/sealed-secrets/#seal-a-secret","text":"Install the kubeseal binary Follow The Usage Directions To Secure a Secret Note: Default helm install requires additional command line arguments to the kubeseal command as follows: kubeseal --controller-name sealed-secrets --controller-namespace sealed-secrets < /tmp/secret.json > sealed-secret.json","title":"Seal A Secret"},{"location":"services/tempo/","text":"Sandbox Service: Tempo Quick Links Project Site Github Repo Helm Chart Background Tempo (from Grafana), is the default distributed tracing store for the Sandbox. It's integrated automatically with the platform Grafana instance for visualizing traces, and receives all traces sent to the Sandbox Opentelemetry Collector instance. Sandbox Customizations The Sandbox installation of loki includes a mostly default installation with the following modifications: Enables a Grafana Datasource config for automatic integration with the cluster Grafana Values can be passed to the official loki chart (by adding them under the \"tempo\" key), as shown on line 3 of the values file here: enableGrafanaDatasource: true tempo: See Customizing Default Services for more information on overriding default values.","title":"Tempo"},{"location":"services/tempo/#sandbox-service-tempo","text":"","title":"Sandbox Service: Tempo"},{"location":"services/tempo/#quick-links","text":"Project Site Github Repo Helm Chart","title":"Quick Links"},{"location":"services/tempo/#background","text":"Tempo (from Grafana), is the default distributed tracing store for the Sandbox. It's integrated automatically with the platform Grafana instance for visualizing traces, and receives all traces sent to the Sandbox Opentelemetry Collector instance.","title":"Background"},{"location":"services/tempo/#sandbox-customizations","text":"The Sandbox installation of loki includes a mostly default installation with the following modifications: Enables a Grafana Datasource config for automatic integration with the cluster Grafana Values can be passed to the official loki chart (by adding them under the \"tempo\" key), as shown on line 3 of the values file here: enableGrafanaDatasource: true tempo: See Customizing Default Services for more information on overriding default values.","title":"Sandbox Customizations"}]}